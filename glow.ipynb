{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNvnEyT--nGW",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "- Generative models are characterized by their ability to learn from limited amounts of data and generalize into varying tasks and contexts.\n",
        "- During recent years, flow-based generative models have attracted particular interest over alternative, traditionally more prominent models due to the tractability of exact inference for latent variables and the exact evaluation of log-likelihood (Kingma & Dhariwal 2018).\n",
        "- In this notebook, a relatively simple instance of glow is implemented and experimented with using CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUEumdBi_J35",
        "colab_type": "text"
      },
      "source": [
        "# Model Specification\n",
        "\n",
        "### Normalizing Flow\n",
        "\n",
        "In the context of majority of flow-based models, the generative process for a high dimensional random vector $\\textbf{x}$ is defined as follows\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "    \\textbf{z} \\sim p_\\theta(\\textbf{z})  \\\\\n",
        "    \\textbf{x} = \\textbf{g}_\\theta(\\textbf{z}).\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "Here $\\textbf{z}$ and $p_\\theta(\\textbf{z})$ is used to denote the latent variable and its corresponding, usually simple, density. Respectively, $\\textbf{g}_\\theta$ is used to denote an invertible function such that the latent variable can be inferred as $\\textbf{z} = \\textbf{g}_\\theta^{-1}(\\textbf{x}) = \\textbf{f}_\\theta(\\textbf{x})$. The functions $\\textbf{f}$ and $\\textbf{g}$ are chosen so that they can be represented as sequences of individual and invertible transformations $\\textbf{f} = \\textbf{f}_1 \\circ \\textbf{f}_2 \\circ \\dots \\circ \\textbf{f}_K$ such that $\\textbf{x} = \\textbf{g}(\\textbf{z})$ can be written as\n",
        "\n",
        "\\begin{equation}\n",
        "    \\textbf{x} \\overset{f_1}{\\leftrightarrow} \\textbf{h}_1 \\overset{f_2}{\\leftrightarrow} \\textbf{h}_2 \\dots  \\overset{f_K}{\\leftrightarrow} \\textbf{z}.\n",
        "\\end{equation}\n",
        "\n",
        "This series of invertible transformations essentially maps a more complex desity $p(\\textbf{x})$ to a simple, tractable density $p(\\textbf{z})$.\n",
        "\n",
        "For continuous data, $\\mathcal{D}$, the log-likelihood objective for normalizing flow models is defined as minimizing\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "    \\mathcal{L}(\\mathcal{D}) \\simeq \\frac{1}{N} \\sum_{i=1}^N -\\log p_{\\theta} (\\hat{x}^{(i)}) + c \\\\\n",
        "    \\hat{x}^{(i)} = x^{(i)} + u, u \\sim \\mathcal{U}(0, a) \\\\\n",
        "    c = -M \\cdot \\log a,\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "i.e. the expected compression cost, where $M$ and $a$ is the dimensionality and discretization level of $\\textbf{x}$. By utilizing the change of variable formula, the probability density $p_\\theta(\\textbf{x})$ can be represented as\n",
        "\n",
        "\\begin{equation}\n",
        "    p_\\theta(\\textbf{x}) = p_\\theta(\\textbf{z}) \\log |\\det(d\\textbf{z}/d\\textbf{x})|| \\\\\n",
        "\\end{equation}\n",
        "\n",
        "By taking a logartihm of the density and representing the transformation $\\textbf{x} \\leftrightarrow \\textbf{z}$ as a series of transformations, we can derive the maximum likelihood objective as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "    \\log p_\\theta(\\textbf{x}) = \\log p_\\theta(\\textbf{z}) + \\log |\\det(d\\textbf{z}/d\\textbf{x})|| \\\\\n",
        "    = \\log p_\\theta(\\textbf{z}) + \\sum_{i=1}^K \\log |\\det(d\\textbf{h}_i/d\\textbf{h}_{i-1})||,\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "Here the terms $|\\det(d\\textbf{h}_i/d\\textbf{h}_{i-1})|$ essentially indicate the expansive/contractive magnitude of a single transformation in sequence. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg0RGCOTWwzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr3Y2rCuWwrA",
        "colab_type": "code",
        "outputId": "c44df525-6be2-4c98-e908-d55769a32c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmNnaX5whNv4",
        "colab_type": "code",
        "outputId": "f0c3ac96-4a0d-483d-8c5f-6b5cc953427f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=5ce26671ac4af5070a886da1175382b3f994cac5b8f196286be5eb566b79206e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 155.7 MB\n",
            "GPU RAM Free: 7611MB | Used: 0MB | Util   0% | Total 7611MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i52NPpTgVFgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "cuda = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi1ki3Y3VFMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "%matplotlib nbagg\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_palette(sns.dark_palette(\"purple\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydro7RmPVFDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "DIM_HIDDEN = 512\n",
        "DEPTH = 10\n",
        "LEVELS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArhqrGj_YMob",
        "colab_type": "code",
        "outputId": "e92b83dc-72f3-458e-d80d-66a6adc8fbfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.transforms import ToTensor\n",
        "from functools import reduce\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "n_bins = 2**8  # 8 bits\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), \n",
        "                         lambda x: x + torch.zeros_like(x).uniform_(0., 1./n_bins)])\n",
        "\n",
        "# Load dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "    \n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=10, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=10, drop_last=True)\n",
        "init_loader = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10(root='./data', train=True, \n",
        "    download=True, transform=transform), batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▊| 168189952/170498071 [00:14<00:00, 13212244.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTORze5pYMl0",
        "colab_type": "code",
        "outputId": "fdc9f1ab-c0b4-49c4-fce5-cbce6bcac4e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "x, _ = trainset[0]\n",
        "x = Variable(x)\n",
        "\n",
        "DIM_CHANNELS, IMG_HEIGHT, IMG_WIDTH = x.shape\n",
        "SHAPE = BATCH_SIZE, DIM_CHANNELS, IMG_HEIGHT, IMG_WIDTH\n",
        "SHAPE"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(64, 3, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq0Ww0vSaPAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn_top = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh5VhhIZZpkJ",
        "colab_type": "text"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1Y9KE1qYMeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/pclucas14/pytorch-glow/blob/master/utils.py\n",
        "def flatten_sum(logps):\n",
        "    while len(logps.size()) > 1: \n",
        "        logps = logps.sum(dim=-1)\n",
        "    return logps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vbJjDjjarZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def onehot(y, num_classes):\n",
        "    y_onehot = torch.zeros(y.size(0), num_classes).to(y.device)\n",
        "    if len(y.size()) == 1:\n",
        "        y_onehot = y_onehot.scatter_(1, y.unsqueeze(-1), 1)\n",
        "    else:\n",
        "        y_onehot = y_onehot.scatter_(1, y, 1)\n",
        "    return y_onehot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GQ-3K2hYMa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian_diag(mean, logsd):\n",
        "    class o(object):\n",
        "        Log2PI = float(np.log(2 * np.pi))\n",
        "        pass\n",
        "\n",
        "        def logps(x):\n",
        "            return  -0.5 * (o.Log2PI + 2. * logsd + (x - mean) ** 2 / torch.exp(2. * logsd))\n",
        "\n",
        "        def sample(eps_std=1):\n",
        "            eps = torch.normal(mean=torch.zeros_like(mean),\n",
        "                           std=torch.ones_like(logsd) * eps_std)\n",
        "            return mean + torch.exp(logsd) * eps\n",
        "\n",
        "    o.logp = lambda x: flatten_sum(o.logps(x))\n",
        "    return o\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnUNXk5hZm1O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard linear layer with zero initial weights and biases\n",
        "\n",
        "class LinearZeros(nn.Linear):\n",
        "    def __init__(self, in_channels, out_channels, logscale=3):\n",
        "        super().__init__(in_channels, out_channels)\n",
        "        self.logscale = logscale\n",
        "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(out_channels)))\n",
        "        # zero init\n",
        "        self.weight.data.zero_()\n",
        "        self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = super().forward(x)\n",
        "        return out * torch.exp(self.logs * self.logscale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XsfbTRrW0a_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard convolutional layer with zero initial weights and biases\n",
        "\n",
        "class ConvZeros(nn.Conv2d):\n",
        "    def __init__(self, channels_in, channels_out, filter_size, stride=1, padding=1, logs_factor=3.):\n",
        "        super().__init__(channels_in, channels_out, filter_size, stride=stride, padding=padding)\n",
        "        self.register_parameter(\"logs\", nn.Parameter(torch.zeros(channels_out, 1, 1)))\n",
        "        self.logs_factor = logs_factor\n",
        "        # Zero init\n",
        "        self.weight.data.zero_()\n",
        "        self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = super().forward(x)\n",
        "        return out * torch.exp(self.logs * self.logs_factor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkdmq7vPMAbY",
        "colab_type": "text"
      },
      "source": [
        "# Glow\n",
        "\n",
        "### Actnorm \n",
        "\n",
        "The actnorm layer performs an affine transformation on the activations using scale and bias parameters per channel. Both scale and shift are initialized depending on the data so that the first batch has a mean of zero and unit variance per channel after the actnorm layer. These parameters are treated as regular trainable parameters after initialization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKFk4qhaZmyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActnormLayer(nn.Module):\n",
        "    def __init__(self, dim_channels, logs_factor=1., scale=1.):\n",
        "        super(ActnormLayer, self).__init__()\n",
        "        self.initialized = False\n",
        "        self.logs_factor = logs_factor\n",
        "        self.scale = scale\n",
        "        self.register_parameter('b', nn.Parameter(torch.zeros(1, dim_channels, 1, 1)))\n",
        "        self.register_parameter('logs', nn.Parameter(torch.zeros(1, dim_channels, 1, 1)))\n",
        "\n",
        "\n",
        "    def init_params(self, x):\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            bias = -torch.mean(x, dim=[0, 2, 3], keepdim=True)\n",
        "            vars = torch.mean((x + bias) ** 2, dim=[0, 2, 3], keepdim=True)\n",
        "            logs = torch.log(self.scale / (torch.sqrt(vars) + 1e-6)) / self.logs_factor\n",
        "\n",
        "            self.b.data.copy_(bias.data)\n",
        "            self.logs.data.copy_(logs.data)\n",
        "\n",
        "\n",
        "    def forward(self, x, reverse, objective):\n",
        "\n",
        "        if not reverse:\n",
        "\n",
        "            if not self.initialized: \n",
        "                self.initialized = True\n",
        "                self.init_params(x)\n",
        "\n",
        "            logs = self.logs * self.logs_factor\n",
        "            b = self.b\n",
        "            \n",
        "            output = (x + b) * torch.exp(logs)\n",
        "            dlogdet = torch.sum(logs) * x.size(2) * x.size(3) # n of pixels \n",
        "\n",
        "            return output.view(x.shape), objective + dlogdet\n",
        "\n",
        "        else:\n",
        "\n",
        "            logs = self.logs * self.logs_factor\n",
        "            b = self.b\n",
        "            output = x * torch.exp(-logs) - b\n",
        "            dlogdet = torch.sum(logs) * x.size(2) * x.size(3) # n of pixels \n",
        "\n",
        "            return output.view(x.shape), objective - dlogdet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAX3BtHJZmrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard convolutional layer with actnorm\n",
        "\n",
        "class ConvActNorm(nn.Module):\n",
        "    def __init__(self, channels_in, channels_out, filter_size, stride=1, padding=None):\n",
        "        super(ConvActNorm, self).__init__()\n",
        "        padding = (filter_size - 1) // 2 or padding\n",
        "        self.conv = nn.Conv2d(channels_in, channels_out, filter_size, padding=padding, bias=False)\n",
        "        self.actnorm =  ActnormLayer(channels_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.actnorm.forward(x, False, -1)[0]\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hjmIgpvMK91",
        "colab_type": "text"
      },
      "source": [
        "### Invertible $1 \\times 1$ convolution\n",
        "\n",
        "As the name suggests, this layer performs an invertible convolution with equal number of input and output channels and a kernel of size $c \\times c$, where for an input tensor of dimensionality $c \\times h \\times w$. This transformation essentially learns to permute the ordering of the channels to account for the fact that affine coupling layers leave the first half of the data untouched. Respectively, the convolution kernel is first initialized as a random rotation matrix.\n",
        "\n",
        "The log-determinant of an invertible $1 \\times 1$ convolution for an input tensor $\\textbf{h}$ is defined as\n",
        "\n",
        "\\begin{equation}\n",
        "    \\log |\\det(\\frac{d \\mathsf{conv2D}(\\textbf{h}; \\textbf{W})}{d \\textbf{h}})| = h \\cdot w \\cdot \\log |\\det(\\textbf{W})|.\n",
        "\\end{equation}\n",
        "\n",
        "The computation time for this log-determinant can be significantly decreased for large values of $c$ by utilizing LU-decomposition for the weight matrix $\\textbf{W}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjK472ZtZmnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Invertible_1x1_Convolution(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim_channels):\n",
        "        super(Invertible_1x1_Convolution, self).__init__()\n",
        "        \n",
        "        self.kernel_shape = 1\n",
        "        self.dim_channels = dim_channels\n",
        "        \n",
        "        # Init with random orthonormal weights\n",
        "        self.register_parameter('w', torch.nn.Parameter(torch.from_numpy(np.linalg.qr(np.random.randn(\n",
        "            *[self.dim_channels,self.dim_channels]))[0].astype('float32'))))\n",
        "        \n",
        "    def forward(self, z, reverse, logdet):\n",
        "        \n",
        "        shape = z.shape\n",
        "        d_logdet = torch.log(torch.abs(torch.det(self.w))) * shape[2] * shape[3]  # pixels\n",
        "        \n",
        "        if not reverse:\n",
        "            \n",
        "            w = self.w\n",
        "            kernel = torch.reshape(w, w.shape + torch.Size([1,1]))\n",
        "            z = F.conv2d(z, kernel)\n",
        "            \n",
        "            return z, logdet + d_logdet\n",
        "\n",
        "        else:\n",
        "\n",
        "            w = torch.inverse(self.w)\n",
        "            kernel = torch.reshape(w, w.shape + torch.Size([1,1]))\n",
        "            z = F.conv2d(z, kernel)\n",
        "            \n",
        "            return z, logdet - d_logdet\n",
        "              "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsPIWmwAMa-e",
        "colab_type": "text"
      },
      "source": [
        "### Affine coupling layer\n",
        "\n",
        "The affine coupling layer first splits the input tensor along its channel dimension after which it performs an affine transformation on the second half of data ($\\textbf{x}_b$) before concatenating them back together. The affine transformation is performed based on the function NN(), which takes the first half of the data ($\\textbf{x}_a$) as an input and outputs the scale and shift parameters for the affine transformation. The function NN() is essentially a shallow convolutional network with its last convolution initialized as zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E670GMbZmkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AffineCouplingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim_channels, dim_hidden=DIM_HIDDEN):\n",
        "        super(AffineCouplingLayer, self).__init__()\n",
        "        \n",
        "        self.dim_channels = dim_channels\n",
        "        \n",
        "        self.out_dim = dim_channels // 2\n",
        "        self.in_dim = dim_channels - self.out_dim\n",
        "        self.dim_hidden = dim_hidden\n",
        "        \n",
        "        self.kernel1 = 3\n",
        "        self.kernel2 = 1\n",
        "        self.kernel3 = 3\n",
        "        \n",
        "        self.nn = nn.Sequential(\n",
        "            ConvActNorm(self.in_dim, self.dim_hidden, self.kernel1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            ConvActNorm(self.dim_hidden, self.dim_hidden, self.kernel2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            ConvZeros(self.dim_hidden, 2*self.out_dim, self.kernel3, padding=1)\n",
        "        )\n",
        "        \n",
        "        \n",
        "    def forward(self, z, reverse, logdet):\n",
        "        \n",
        "        z_a, z_b = torch.chunk(z, 2, dim=1)\n",
        "        \n",
        "        out = self.nn(z_a)\n",
        "        \n",
        "        shift, log_scale = out[:, 0::2], out[:, 1::2]\n",
        "        scale = torch.sigmoid(log_scale + 2.)\n",
        "        \n",
        "        if not reverse:\n",
        "            z_b += shift\n",
        "            z_b *= scale\n",
        "            logdet += flatten_sum(torch.log(scale))\n",
        "            \n",
        "        else:\n",
        "            z_b /= scale\n",
        "            z_b -= shift\n",
        "            logdet -= flatten_sum(torch.log(scale))\n",
        "            \n",
        "          \n",
        "        z = torch.cat([z_a, z_b], 1)\n",
        "        \n",
        "        return z, logdet\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYkgJSK2TSxn",
        "colab_type": "text"
      },
      "source": [
        "### Multi-scale architecture\n",
        "\n",
        "The multi-scale architecture of the model consists of sequences of flow steps, i.e.\n",
        "\n",
        "- **Step of flow**: actnorm $\\rightarrow$ invertible $1 \\times 1$ convolution $\\rightarrow$ affine coupling\n",
        "\n",
        "preceded by a squeeze operation (transforming a $c \\times h \\times w$ tensor into shape of $4c \\times \\frac{h}{2} \\times \\frac{w}{2}$) and followed by a split layer. \n",
        "\n",
        "- **Level of flow**: squeeze $\\rightarrow$ step of flow $\\rightarrow$ split\n",
        "\n",
        "At each split layer, half of the dimensions are factored out and modelled as a Gaussian distribution (optionally storing the cut-off dimensions) while the other half are selected for further layers as described in the work of Dinh et al. (2016). A complete glow consists of a stack of multiple levels of flow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsuUdTpHZmiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlowStep(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim_channels):\n",
        "        super(FlowStep, self).__init__()\n",
        "        \n",
        "        self.actnorm = ActnormLayer(dim_channels)\n",
        "        self.invertible_convolution = Invertible_1x1_Convolution(dim_channels)\n",
        "        self.affine_coupling = AffineCouplingLayer(dim_channels)\n",
        "        \n",
        "        \n",
        "    def forward(self, z, reverse, logdet):\n",
        "        \n",
        "        if not reverse:\n",
        "            \n",
        "            #print('Initial: {}'.format(logdet[0]))\n",
        "            z, logdet = self.actnorm(z, reverse, logdet)\n",
        "            #print('Actnorm: {}'.format(logdet[0]))\n",
        "            z, logdet = self.invertible_convolution(z, reverse, logdet)\n",
        "            #print('Conv: {}'.format(logdet[0]))\n",
        "            z, logdet = self.affine_coupling(z, reverse, logdet)\n",
        "            #print('Affine: {}'.format(logdet[0]))\n",
        "             \n",
        "        else:\n",
        "            \n",
        "            z, logdet = self.affine_coupling(z, reverse, logdet)\n",
        "            z, logdet = self.invertible_convolution(z, reverse, logdet)\n",
        "            z, logdet = self.actnorm(z, reverse, logdet)\n",
        "        \n",
        "        return z, logdet\n",
        "                      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIzN1MsnZmf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SqueezeLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, factor=2):\n",
        "        super(SqueezeLayer, self).__init__()\n",
        "        \n",
        "        self.factor = factor\n",
        "\n",
        "    def squeeze(self, x):\n",
        "        batch_size, dim_channel, height, width = x.shape\n",
        "\n",
        "        # https://github.com/chaiyujin/glow-pytorch/blob/master/glow/modules.py\n",
        "        x = x.view(batch_size, dim_channel, height // self.factor, self.factor, width // self.factor, self.factor)\n",
        "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
        "        x = x.view(batch_size, dim_channel * self.factor ** 2, height // self.factor, width // self.factor)\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def unsqueeze(self, x):\n",
        "        batch_size, dim_channel, height, width = x.shape\n",
        "\n",
        "        # https://github.com/chaiyujin/glow-pytorch/blob/master/glow/modules.py\n",
        "        x = x.view(batch_size, dim_channel // self.factor ** 2, self.factor, self.factor, height, width)\n",
        "        x = x.permute(0, 1, 4, 2, 5, 3).contiguous()\n",
        "        x = x.view(batch_size, dim_channel // self.factor ** 2, height * self.factor, width * self.factor)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def forward(self, z, reverse, logdet):\n",
        "        \n",
        "        if not reverse:\n",
        "            return self.squeeze(z), logdet\n",
        "               \n",
        "        else:\n",
        "            return self.unsqueeze(z), logdet\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l534K6T9Zmdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SplitLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, dim_channels):\n",
        "        super(SplitLayer, self).__init__()\n",
        "        self.dim_channels = dim_channels\n",
        "        self.in_dim = dim_channels // 2\n",
        "        \n",
        "        self.conv = ConvZeros(self.in_dim, self.dim_channels, 3, padding=1)\n",
        "\n",
        "    def prior(self, z):\n",
        "        out = self.conv(z)\n",
        "        shift, log_scale = out[:, 0::2], out[:, 1::2]\n",
        "        return gaussian_diag(shift, log_scale)\n",
        "\n",
        "    def forward(self, z, reverse, logdet, eps_std=1, use_stored_sample=False):\n",
        "        \n",
        "        if not reverse:\n",
        "            \n",
        "            z_a, z_b = torch.chunk(z, 2, dim=1)\n",
        "            \n",
        "            pz = self.prior(z_a)\n",
        "            self.sample = z_b\n",
        "            \n",
        "            logdet += pz.logp(z_b)\n",
        "            \n",
        "            return z_a, logdet\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            pz = self.prior(z)\n",
        "            z_b = self.sample if use_stored_sample else pz.sample(eps_std)\n",
        "            # Concatenate the input and sample\n",
        "            z = torch.cat([z, z_b], dim=1)\n",
        "            logdet -= pz.logp(z_b)\n",
        "            \n",
        "            return z, logdet\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjxLZ-3tZmbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Prior(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_shape):\n",
        "        super(Prior, self).__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.conv = ConvZeros(2*input_shape[1], 2*input_shape[1], 3, padding=1)\n",
        "        self.project_y = LinearZeros(len(classes), 2 * self.input_shape[1])\n",
        "        self.register_parameter(\"prior\",\n",
        "            nn.Parameter(torch.zeros([input_shape[0], 2*input_shape[1], input_shape[2], input_shape[3]])))\n",
        "\n",
        "        \n",
        "    def forward(self, z, reverse, logdet, y_onehot=None, eps_std=1):\n",
        "\n",
        "        B, C = self.prior.size(0), self.prior.size(1)\n",
        "        \n",
        "        if z is not None:\n",
        "            b, c, h, w = z.shape\n",
        "        else:\n",
        "            b, c, h, w = self.input_shape\n",
        "        \n",
        "        h_ = self.prior.detach().clone()\n",
        "\n",
        "        if learn_top:\n",
        "            h_ = self.conv(h_)\n",
        "\n",
        "        if y_onehot is not None:\n",
        "            y_proj = self.project_y(y_onehot).view(B, C, 1, 1)\n",
        "            h_ += y_proj\n",
        "\n",
        "        mean, logsd = torch.chunk(h_, 2, dim=1)\n",
        "        pz = gaussian_diag(mean, logsd)\n",
        "\n",
        "        if not reverse:\n",
        "        \n",
        "            logdet += pz.logp(z).cuda()\n",
        "            return z, logdet\n",
        "\n",
        "        else:\n",
        "        \n",
        "            if z is None:\n",
        "                z = pz.sample(eps_std)\n",
        "            logdet -= pz.logp(z).cuda()\n",
        "            return z, logdet\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLw2MYmjZmYv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Glow(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_shape=SHAPE, levels=LEVELS, depth=DEPTH):\n",
        "        super(Glow, self).__init__()\n",
        "        \n",
        "        self.input_shape = input_shape\n",
        "        \n",
        "        b, c, h, w = input_shape\n",
        "        layers = []\n",
        "        self.layer_names = []\n",
        "        self.out_shapes = []\n",
        "        \n",
        "        for l in range(levels):\n",
        "            layers += [SqueezeLayer()]\n",
        "            self.layer_names += ['Squeeze']\n",
        "            c, h, w = c * 4, h // 2, w // 2\n",
        "            self.out_shapes.append([-1, c, h, w])\n",
        "            \n",
        "            for _ in range(depth):\n",
        "                layers += [FlowStep(c)]\n",
        "                self.layer_names += ['Flow']\n",
        "                self.out_shapes.append([-1, c, h, w])\n",
        "                \n",
        "            if l < levels - 1:\n",
        "                layers += [SplitLayer(c)]\n",
        "                self.layer_names += ['Split']\n",
        "                c //= 2\n",
        "                self.out_shapes.append([-1, c, h, w])\n",
        "        \n",
        "        layers += [Prior([b,c,h,w])]\n",
        "        self.layer_names += ['Prior']\n",
        "        \n",
        "        self.layers =  nn.ModuleList(layers)\n",
        "        self.project_class = LinearZeros(self.out_shapes[-1][1], len(classes))\n",
        "\n",
        "        \n",
        "    def forward(self, x, reverse, logdet, y_onehot, eps_std=1):\n",
        "        \n",
        "        if not reverse:\n",
        "            \n",
        "            #it = time.time()\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                \n",
        "                #t = time.time()\n",
        "                \n",
        "                if isinstance(layer, SplitLayer):\n",
        "                    x, logdet = layer(x, reverse, logdet, eps_std)\n",
        "                elif isinstance(layer, Prior):\n",
        "                    x, logdet = layer(x, reverse, logdet, y_onehot, eps_std)\n",
        "                else:                \n",
        "                    x, logdet = layer(x, reverse, logdet)\n",
        "\n",
        "                #print(\"{}: {}\".format(self.layer_names[i], time.time() - t))\n",
        "\n",
        "            #print(time.time() - it)\n",
        "\n",
        "        else:\n",
        "            \n",
        "            with torch.no_grad():\n",
        "\n",
        "                for i, layer in enumerate(reversed(self.layers)):\n",
        "                    \n",
        "                    if isinstance(layer, SplitLayer):\n",
        "                        x, logdet = layer(x, reverse, logdet, eps_std)\n",
        "                    elif isinstance(layer, Prior):\n",
        "                        x, logdet = layer(x, reverse, logdet, y_onehot, eps_std)\n",
        "                    else:                \n",
        "                        x, logdet = layer(x, reverse, logdet)\n",
        "\n",
        "                \n",
        "        return x, logdet\n",
        "        \n",
        "        \n",
        "    def sample(self, y_onehot=None, eps_std=1):\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            samples = self.forward(None, True, torch.zeros(self.input_shape[0]).cuda(), y_onehot, eps_std)[0]\n",
        "    \n",
        "            return samples\n",
        "\n",
        "      \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_O5LxOCZmWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEBlCYerahI1",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJF4TU9oZmUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Glow().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXkLJpVtZmRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "# init learning rate\n",
        "lr = 1e-3\n",
        "\n",
        "optim = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=45, gamma=0.1)\n",
        "BCE = nn.BCEWithLogitsLoss().cuda()\n",
        "\n",
        "\n",
        "def objective(img_shape, logdet):\n",
        "    l = (-logdet) / float(np.log(2.) * np.prod(img_shape[1:]))\n",
        "    return torch.mean(l)\n",
        "\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0NJ624NZmO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data dependent init\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for (img, y) in init_loader:\n",
        "        img = img.cuda()\n",
        "        y_onehot = onehot(y, len(classes)).cuda()\n",
        "        logdet = torch.zeros_like(img[:, 0, 0, 0])\n",
        "        _ = model(img, False, logdet, y_onehot)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5jkXqe6c_tD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.DataParallel(model).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLFDn3LuZmMY",
        "colab_type": "code",
        "outputId": "95e85ddb-a5dd-4642-def1-facac4ffd04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 2000\n",
        "num_warmup_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch: {}'.format(epoch))\n",
        "    num_batches = len(train_loader)\n",
        "    avg_train_bits_x = 0.\n",
        "\n",
        "    for i, (img, y) in enumerate(train_loader):\n",
        "        #if i > 10 : break\n",
        "            \n",
        "        t = time.time()\n",
        "        img = img.cuda()\n",
        "        y_onehot = onehot(y, len(classes)).cuda()\n",
        "        logdet = torch.zeros_like(img[:, 0, 0, 0]).cuda()\n",
        "        #print(logdet)\n",
        "        # discretizing cost \n",
        "        logdet += float(-np.log(n_bins) * np.prod(img.shape[1:]))\n",
        "        #print(logdet)\n",
        "        # log_det_jacobian cost (and some prior from Split OP)\n",
        "        z, logdet = model(img, False, logdet, y_onehot)\n",
        "        #print(logdet)\n",
        "        loss_generative = objective(img.shape, logdet)\n",
        "\n",
        "        y_logits = model.module.project_class(z.mean(2).mean(2))\n",
        "        loss_classes = BCE(y_logits, y_onehot)\n",
        "\n",
        "        loss = loss_generative #+ loss_classes * 0.5\n",
        "\n",
        "        #ot = time.time()\n",
        "\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_value_(model.parameters(), 5)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 100)\n",
        "\n",
        "        optim.step()\n",
        "\n",
        "        #print('Optimization time: {}'.format(time.time() - ot))\n",
        "\n",
        "        avg_train_bits_x += loss.item()\n",
        "\n",
        "        # update learning rate\n",
        "        new_lr = float(lr * min(1., (i + epoch * num_batches) / (num_warmup_epochs * num_batches)))\n",
        "        for pg in optim.param_groups: pg['lr'] = new_lr\n",
        "\n",
        "        if (i + 1) % 200 == 0: \n",
        "            print('avg train bits per pixel {:.4f}'.format(avg_train_bits_x / 200))\n",
        "            avg_train_bits_x = 0.\n",
        "            sample = model.module.sample(y_onehot)\n",
        "            grid = utils.make_grid(sample)\n",
        "\n",
        "            utils.save_image(grid, '/content/gdrive/My Drive/Colab Notebooks/glow_samples/samples2/cifar_Test_{}_{}.png'.format(epoch, i // 200))\n",
        "            #files.download('cifar_Test_{}_{}.png'.format(epoch, i // 500))\n",
        "\n",
        "        print('Batch: {}, iteration took {:.4f}, Loss: {}'.format(i, time.time() - t, loss))\n",
        "\n",
        "\n",
        "    if (epoch + 1) % 1 == 0:\n",
        "        model.eval()\n",
        "        avg_test_bits_x = 0.\n",
        "        with torch.no_grad():\n",
        "            for i, (img, y) in enumerate(test_loader): \n",
        "                #if i > 10 : break\n",
        "                img = img.cuda() \n",
        "                y_onehot = onehot(y, len(classes)).cuda()\n",
        "\n",
        "                logdet = torch.zeros_like(img[:, 0, 0, 0]).cuda()\n",
        "               \n",
        "                # discretizing cost \n",
        "                logdet += float(-np.log(n_bins) * np.prod(img.shape[1:]))\n",
        "                \n",
        "                # log_det_jacobian cost (and some prior from Split OP)\n",
        "                z, logdet = model(img, False, logdet, y_onehot)\n",
        "                last_img = img\n",
        "\n",
        "                loss_generative = objective(img.shape, logdet)\n",
        "\n",
        "                y_logits = model.module.project_class(z.mean(2).mean(2))\n",
        "                loss_classes = BCE(y_logits, y_onehot)\n",
        "\n",
        "                loss = loss_generative #+ loss_classes * 0.5\n",
        "\n",
        "                avg_test_bits_x += loss\n",
        "\n",
        "            print('avg test bits per pixel {:.4f}'.format(avg_test_bits_x.item() / i))\n",
        "\n",
        "            sample = model.module.sample(y_onehot)\n",
        "            grid = utils.make_grid(sample)\n",
        "\n",
        "            utils.save_image(grid, '/content/gdrive/My Drive/Colab Notebooks/glow_samples/samples2/cifar_Test_{}.png'.format(epoch))\n",
        "            #files.download('cifar_Test_{}.png'.format(epoch))\n",
        "\n",
        "            # reconstruct\n",
        "            x_hat = model.module(z, True, logdet, y_onehot)[0]\n",
        "            grid = utils.make_grid(x_hat)\n",
        "\n",
        "            utils.save_image(grid, '/content/gdrive/My Drive/Colab Notebooks/glow_samples/samples2/cifar_Test_Recon{}.png'.format(epoch))\n",
        "            #files.download('cifar_Test_Recon{}.png'.format(epoch))\n",
        "        \n",
        "            grid = utils.make_grid(last_img)\n",
        "\n",
        "            utils.save_image(grid, '/content/gdrive/My Drive/Colab Notebooks/glow_samples/samples2/cifar_Test_Target.png')\n",
        "            #files.download('cifar_Test_Target.png')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "Batch: 0, iteration took 0.7629, Loss: 6.716815948486328\n",
            "Batch: 1, iteration took 0.4955, Loss: 12.63048267364502\n",
            "Batch: 2, iteration took 0.4720, Loss: 12.299488067626953\n",
            "Batch: 3, iteration took 0.4747, Loss: 12.603134155273438\n",
            "Batch: 4, iteration took 0.4695, Loss: 12.544177055358887\n",
            "Batch: 5, iteration took 0.4727, Loss: 12.407720565795898\n",
            "Batch: 6, iteration took 0.4639, Loss: 12.437883377075195\n",
            "Batch: 7, iteration took 0.4679, Loss: 12.537416458129883\n",
            "Batch: 8, iteration took 0.4639, Loss: 12.624016761779785\n",
            "Batch: 9, iteration took 0.4542, Loss: 12.287006378173828\n",
            "Batch: 10, iteration took 0.4798, Loss: 12.630977630615234\n",
            "Batch: 11, iteration took 0.4480, Loss: 12.540946006774902\n",
            "Batch: 12, iteration took 0.4771, Loss: 12.725553512573242\n",
            "Batch: 13, iteration took 0.4537, Loss: 12.163455963134766\n",
            "Batch: 14, iteration took 0.4719, Loss: 12.271730422973633\n",
            "Batch: 15, iteration took 0.4543, Loss: 12.080949783325195\n",
            "Batch: 16, iteration took 0.4662, Loss: 12.16282844543457\n",
            "Batch: 17, iteration took 0.4904, Loss: 12.322510719299316\n",
            "Batch: 18, iteration took 0.4497, Loss: 11.825189590454102\n",
            "Batch: 19, iteration took 0.4746, Loss: 11.656377792358398\n",
            "Batch: 20, iteration took 0.4888, Loss: 12.289705276489258\n",
            "Batch: 21, iteration took 0.4599, Loss: 11.731521606445312\n",
            "Batch: 22, iteration took 0.4708, Loss: 11.60775089263916\n",
            "Batch: 23, iteration took 0.4633, Loss: 11.508639335632324\n",
            "Batch: 24, iteration took 0.4537, Loss: 11.65010929107666\n",
            "Batch: 25, iteration took 0.4611, Loss: 11.481734275817871\n",
            "Batch: 26, iteration took 0.4803, Loss: 11.71683120727539\n",
            "Batch: 27, iteration took 0.4795, Loss: 10.947397232055664\n",
            "Batch: 28, iteration took 0.4893, Loss: 11.200469970703125\n",
            "Batch: 29, iteration took 0.4808, Loss: 11.013381958007812\n",
            "Batch: 30, iteration took 0.4695, Loss: 10.877534866333008\n",
            "Batch: 31, iteration took 0.4730, Loss: 11.09577751159668\n",
            "Batch: 32, iteration took 0.4744, Loss: 10.700247764587402\n",
            "Batch: 33, iteration took 0.4790, Loss: 10.85696792602539\n",
            "Batch: 34, iteration took 0.4951, Loss: 10.622394561767578\n",
            "Batch: 35, iteration took 0.4829, Loss: 10.604032516479492\n",
            "Batch: 36, iteration took 0.4753, Loss: 10.51905632019043\n",
            "Batch: 37, iteration took 0.4716, Loss: 10.709585189819336\n",
            "Batch: 38, iteration took 0.4888, Loss: 10.5950288772583\n",
            "Batch: 39, iteration took 0.4847, Loss: 10.421178817749023\n",
            "Batch: 40, iteration took 0.4596, Loss: 10.328049659729004\n",
            "Batch: 41, iteration took 0.4958, Loss: 10.453057289123535\n",
            "Batch: 42, iteration took 0.4611, Loss: 10.349056243896484\n",
            "Batch: 43, iteration took 0.4796, Loss: 10.120676040649414\n",
            "Batch: 44, iteration took 0.4740, Loss: 9.936145782470703\n",
            "Batch: 45, iteration took 0.4735, Loss: 9.904251098632812\n",
            "Batch: 46, iteration took 0.4866, Loss: 9.997417449951172\n",
            "Batch: 47, iteration took 0.4853, Loss: 10.027105331420898\n",
            "Batch: 48, iteration took 0.4716, Loss: 10.013765335083008\n",
            "Batch: 49, iteration took 0.4576, Loss: 9.675889015197754\n",
            "Batch: 50, iteration took 0.4810, Loss: 9.725080490112305\n",
            "Batch: 51, iteration took 0.4546, Loss: 9.446073532104492\n",
            "Batch: 52, iteration took 0.4678, Loss: 9.804800033569336\n",
            "Batch: 53, iteration took 0.4690, Loss: 9.418718338012695\n",
            "Batch: 54, iteration took 0.4744, Loss: 9.434625625610352\n",
            "Batch: 55, iteration took 0.4780, Loss: 9.413481712341309\n",
            "Batch: 56, iteration took 0.4640, Loss: 9.494620323181152\n",
            "Batch: 57, iteration took 0.4854, Loss: 9.362408638000488\n",
            "Batch: 58, iteration took 0.4465, Loss: 9.166592597961426\n",
            "Batch: 59, iteration took 0.4777, Loss: 9.329530715942383\n",
            "Batch: 60, iteration took 0.4586, Loss: 9.19853401184082\n",
            "Batch: 61, iteration took 0.4521, Loss: 9.216878890991211\n",
            "Batch: 62, iteration took 0.4795, Loss: 9.114277839660645\n",
            "Batch: 63, iteration took 0.4670, Loss: 8.96634292602539\n",
            "Batch: 64, iteration took 0.4695, Loss: 9.146180152893066\n",
            "Batch: 65, iteration took 0.4661, Loss: 9.03925895690918\n",
            "Batch: 66, iteration took 0.4638, Loss: 9.086170196533203\n",
            "Batch: 67, iteration took 0.4591, Loss: 9.172975540161133\n",
            "Batch: 68, iteration took 0.4775, Loss: 8.800440788269043\n",
            "Batch: 69, iteration took 0.4697, Loss: 8.914794921875\n",
            "Batch: 70, iteration took 0.4731, Loss: 8.946012496948242\n",
            "Batch: 71, iteration took 0.4830, Loss: 8.832839965820312\n",
            "Batch: 72, iteration took 0.4690, Loss: 8.6906099319458\n",
            "Batch: 73, iteration took 0.4766, Loss: 8.742833137512207\n",
            "Batch: 74, iteration took 0.4662, Loss: 8.739856719970703\n",
            "Batch: 75, iteration took 0.4831, Loss: 8.829957962036133\n",
            "Batch: 76, iteration took 0.4659, Loss: 8.686761856079102\n",
            "Batch: 77, iteration took 0.4827, Loss: 8.675552368164062\n",
            "Batch: 78, iteration took 0.4663, Loss: 8.506985664367676\n",
            "Batch: 79, iteration took 0.4742, Loss: 8.504417419433594\n",
            "Batch: 80, iteration took 0.4646, Loss: 8.498090744018555\n",
            "Batch: 81, iteration took 0.4749, Loss: 8.45773696899414\n",
            "Batch: 82, iteration took 0.4642, Loss: 8.60413646697998\n",
            "Batch: 83, iteration took 0.4652, Loss: 8.34301471710205\n",
            "Batch: 84, iteration took 0.4895, Loss: 8.302865028381348\n",
            "Batch: 85, iteration took 0.4536, Loss: 8.298173904418945\n",
            "Batch: 86, iteration took 0.4846, Loss: 8.296335220336914\n",
            "Batch: 87, iteration took 0.4506, Loss: 8.244943618774414\n",
            "Batch: 88, iteration took 0.4761, Loss: 8.352127075195312\n",
            "Batch: 89, iteration took 0.4589, Loss: 8.287641525268555\n",
            "Batch: 90, iteration took 0.4723, Loss: 8.105661392211914\n",
            "Batch: 91, iteration took 0.4836, Loss: 8.054272651672363\n",
            "Batch: 92, iteration took 0.4750, Loss: 8.146679878234863\n",
            "Batch: 93, iteration took 0.4814, Loss: 8.24716567993164\n",
            "Batch: 94, iteration took 0.4570, Loss: 8.065645217895508\n",
            "Batch: 95, iteration took 0.4777, Loss: 8.039483070373535\n",
            "Batch: 96, iteration took 0.4548, Loss: 8.020742416381836\n",
            "Batch: 97, iteration took 0.4522, Loss: 8.189801216125488\n",
            "Batch: 98, iteration took 0.4603, Loss: 7.977889537811279\n",
            "Batch: 99, iteration took 0.4586, Loss: 7.9822797775268555\n",
            "Batch: 100, iteration took 0.4770, Loss: 7.945342063903809\n",
            "Batch: 101, iteration took 0.4837, Loss: 7.932051658630371\n",
            "Batch: 102, iteration took 0.4744, Loss: 7.850064277648926\n",
            "Batch: 103, iteration took 0.4771, Loss: 7.911095142364502\n",
            "Batch: 104, iteration took 0.4807, Loss: 7.860682010650635\n",
            "Batch: 105, iteration took 0.4843, Loss: 7.777996063232422\n",
            "Batch: 106, iteration took 0.4648, Loss: 7.76750373840332\n",
            "Batch: 107, iteration took 0.4640, Loss: 7.793279647827148\n",
            "Batch: 108, iteration took 0.4632, Loss: 7.718289852142334\n",
            "Batch: 109, iteration took 0.4783, Loss: 7.71257209777832\n",
            "Batch: 110, iteration took 0.4746, Loss: 7.671629905700684\n",
            "Batch: 111, iteration took 0.4744, Loss: 7.694393157958984\n",
            "Batch: 112, iteration took 0.4764, Loss: 7.563681602478027\n",
            "Batch: 113, iteration took 0.4892, Loss: 7.59859037399292\n",
            "Batch: 114, iteration took 0.4682, Loss: 7.55159854888916\n",
            "Batch: 115, iteration took 0.4811, Loss: 7.622642517089844\n",
            "Batch: 116, iteration took 0.4752, Loss: 7.509484767913818\n",
            "Batch: 117, iteration took 0.4807, Loss: 7.468770980834961\n",
            "Batch: 118, iteration took 0.4740, Loss: 7.547804832458496\n",
            "Batch: 119, iteration took 0.4738, Loss: 7.412774085998535\n",
            "Batch: 120, iteration took 0.4970, Loss: 7.453376293182373\n",
            "Batch: 121, iteration took 0.4932, Loss: 7.465562343597412\n",
            "Batch: 122, iteration took 0.4674, Loss: 7.51381254196167\n",
            "Batch: 123, iteration took 0.4681, Loss: 7.3495025634765625\n",
            "Batch: 124, iteration took 0.4724, Loss: 7.420080184936523\n",
            "Batch: 125, iteration took 0.4510, Loss: 7.311543941497803\n",
            "Batch: 126, iteration took 0.4808, Loss: 7.286983966827393\n",
            "Batch: 127, iteration took 0.4812, Loss: 7.321516513824463\n",
            "Batch: 128, iteration took 0.4826, Loss: 7.312761306762695\n",
            "Batch: 129, iteration took 0.4554, Loss: 7.23028564453125\n",
            "Batch: 130, iteration took 0.4770, Loss: 7.220025062561035\n",
            "Batch: 131, iteration took 0.4755, Loss: 7.254698753356934\n",
            "Batch: 132, iteration took 0.4677, Loss: 7.186412334442139\n",
            "Batch: 133, iteration took 0.4703, Loss: 7.1586809158325195\n",
            "Batch: 134, iteration took 0.5054, Loss: 7.163942337036133\n",
            "Batch: 135, iteration took 0.4744, Loss: 7.141491889953613\n",
            "Batch: 136, iteration took 0.4572, Loss: 7.174477577209473\n",
            "Batch: 137, iteration took 0.4571, Loss: 7.143087387084961\n",
            "Batch: 138, iteration took 0.4837, Loss: 7.038500785827637\n",
            "Batch: 139, iteration took 0.4652, Loss: 7.040454864501953\n",
            "Batch: 140, iteration took 0.4552, Loss: 6.97084903717041\n",
            "Batch: 141, iteration took 0.4529, Loss: 6.975879669189453\n",
            "Batch: 142, iteration took 0.4768, Loss: 7.023774147033691\n",
            "Batch: 143, iteration took 0.4798, Loss: 6.911950588226318\n",
            "Batch: 144, iteration took 0.4621, Loss: 6.986755847930908\n",
            "Batch: 145, iteration took 0.4714, Loss: 6.905951023101807\n",
            "Batch: 146, iteration took 0.4544, Loss: 6.956666946411133\n",
            "Batch: 147, iteration took 0.4840, Loss: 6.952328681945801\n",
            "Batch: 148, iteration took 0.4831, Loss: 6.892658710479736\n",
            "Batch: 149, iteration took 0.4640, Loss: 6.897083282470703\n",
            "Batch: 150, iteration took 0.4510, Loss: 6.878091812133789\n",
            "Batch: 151, iteration took 0.4724, Loss: 6.90641975402832\n",
            "Batch: 152, iteration took 0.4789, Loss: 6.789063453674316\n",
            "Batch: 153, iteration took 0.4552, Loss: 6.835197448730469\n",
            "Batch: 154, iteration took 0.4542, Loss: 6.858160018920898\n",
            "Batch: 155, iteration took 0.4774, Loss: 6.734220504760742\n",
            "Batch: 156, iteration took 0.4768, Loss: 6.8437395095825195\n",
            "Batch: 157, iteration took 0.4762, Loss: 6.771416187286377\n",
            "Batch: 158, iteration took 0.4669, Loss: 6.7766523361206055\n",
            "Batch: 159, iteration took 0.4897, Loss: 6.666421890258789\n",
            "Batch: 160, iteration took 0.4627, Loss: 6.7043352127075195\n",
            "Batch: 161, iteration took 0.4702, Loss: 6.668760776519775\n",
            "Batch: 162, iteration took 0.4611, Loss: 6.749495506286621\n",
            "Batch: 163, iteration took 0.4514, Loss: 6.691841125488281\n",
            "Batch: 164, iteration took 0.4524, Loss: 6.662996292114258\n",
            "Batch: 165, iteration took 0.4793, Loss: 6.611863136291504\n",
            "Batch: 166, iteration took 0.4719, Loss: 6.570882797241211\n",
            "Batch: 167, iteration took 0.4678, Loss: 6.633840084075928\n",
            "Batch: 168, iteration took 0.4743, Loss: 6.600537300109863\n",
            "Batch: 169, iteration took 0.4548, Loss: 6.541569232940674\n",
            "Batch: 170, iteration took 0.4726, Loss: 6.532192230224609\n",
            "Batch: 171, iteration took 0.4714, Loss: 6.560233116149902\n",
            "Batch: 172, iteration took 0.4582, Loss: 6.589895248413086\n",
            "Batch: 173, iteration took 0.4636, Loss: 6.554816246032715\n",
            "Batch: 174, iteration took 0.4887, Loss: 6.488506317138672\n",
            "Batch: 175, iteration took 0.4580, Loss: 6.5457563400268555\n",
            "Batch: 176, iteration took 0.4478, Loss: 6.483473777770996\n",
            "Batch: 177, iteration took 0.4528, Loss: 6.495922088623047\n",
            "Batch: 178, iteration took 0.4497, Loss: 6.469696521759033\n",
            "Batch: 179, iteration took 0.4619, Loss: 6.4508466720581055\n",
            "Batch: 180, iteration took 0.4515, Loss: 6.484836578369141\n",
            "Batch: 181, iteration took 0.4506, Loss: 6.4229512214660645\n",
            "Batch: 182, iteration took 0.4687, Loss: 6.445193290710449\n",
            "Batch: 183, iteration took 0.4582, Loss: 6.437070846557617\n",
            "Batch: 184, iteration took 0.4593, Loss: 6.387216091156006\n",
            "Batch: 185, iteration took 0.4778, Loss: 6.410521984100342\n",
            "Batch: 186, iteration took 0.4819, Loss: 6.336085319519043\n",
            "Batch: 187, iteration took 0.4679, Loss: 6.387970924377441\n",
            "Batch: 188, iteration took 0.4621, Loss: 6.353456497192383\n",
            "Batch: 189, iteration took 0.4537, Loss: 6.351128578186035\n",
            "Batch: 190, iteration took 0.4746, Loss: 6.310798645019531\n",
            "Batch: 191, iteration took 0.4779, Loss: 6.299626350402832\n",
            "Batch: 192, iteration took 0.4851, Loss: 6.335848808288574\n",
            "Batch: 193, iteration took 0.4611, Loss: 6.320303916931152\n",
            "Batch: 194, iteration took 0.4596, Loss: 6.2507171630859375\n",
            "Batch: 195, iteration took 0.4620, Loss: 6.281279563903809\n",
            "Batch: 196, iteration took 0.4733, Loss: 6.271233558654785\n",
            "Batch: 197, iteration took 0.4634, Loss: 6.268796920776367\n",
            "Batch: 198, iteration took 0.4738, Loss: 6.2957305908203125\n",
            "avg train bits per pixel 8.4822\n",
            "Batch: 199, iteration took 2.5222, Loss: 6.21201229095459\n",
            "Batch: 200, iteration took 0.4741, Loss: 6.240530967712402\n",
            "Batch: 201, iteration took 0.4626, Loss: 6.22175931930542\n",
            "Batch: 202, iteration took 0.4463, Loss: 6.217685699462891\n",
            "Batch: 203, iteration took 0.4858, Loss: 6.197230339050293\n",
            "Batch: 204, iteration took 0.4568, Loss: 6.18735408782959\n",
            "Batch: 205, iteration took 0.4739, Loss: 6.186709403991699\n",
            "Batch: 206, iteration took 0.4764, Loss: 6.1666083335876465\n",
            "Batch: 207, iteration took 0.4775, Loss: 6.193487644195557\n",
            "Batch: 208, iteration took 0.4765, Loss: 6.182602882385254\n",
            "Batch: 209, iteration took 0.4724, Loss: 6.064095497131348\n",
            "Batch: 210, iteration took 0.4661, Loss: 6.136878967285156\n",
            "Batch: 211, iteration took 0.4727, Loss: 6.10355806350708\n",
            "Batch: 212, iteration took 0.4677, Loss: 6.139218807220459\n",
            "Batch: 213, iteration took 0.4558, Loss: 6.085870742797852\n",
            "Batch: 214, iteration took 0.4685, Loss: 6.08860445022583\n",
            "Batch: 215, iteration took 0.4507, Loss: 6.091146469116211\n",
            "Batch: 216, iteration took 0.4607, Loss: 6.046747207641602\n",
            "Batch: 217, iteration took 0.4804, Loss: 6.070976257324219\n",
            "Batch: 218, iteration took 0.4712, Loss: 6.076905727386475\n",
            "Batch: 219, iteration took 0.4801, Loss: 6.040644645690918\n",
            "Batch: 220, iteration took 0.4757, Loss: 6.008066177368164\n",
            "Batch: 221, iteration took 0.4821, Loss: 6.012909889221191\n",
            "Batch: 222, iteration took 0.4660, Loss: 6.010458469390869\n",
            "Batch: 223, iteration took 0.4716, Loss: 6.03563117980957\n",
            "Batch: 224, iteration took 0.4582, Loss: 5.998362064361572\n",
            "Batch: 225, iteration took 0.4509, Loss: 5.9957475662231445\n",
            "Batch: 226, iteration took 0.4454, Loss: 5.94666862487793\n",
            "Batch: 227, iteration took 0.4741, Loss: 5.975212097167969\n",
            "Batch: 228, iteration took 0.4794, Loss: 5.981323719024658\n",
            "Batch: 229, iteration took 0.4787, Loss: 5.960059642791748\n",
            "Batch: 230, iteration took 0.4629, Loss: 5.876224994659424\n",
            "Batch: 231, iteration took 0.4546, Loss: 5.952118873596191\n",
            "Batch: 232, iteration took 0.4721, Loss: 5.9061994552612305\n",
            "Batch: 233, iteration took 0.4779, Loss: 5.936002731323242\n",
            "Batch: 234, iteration took 0.4801, Loss: 5.920948028564453\n",
            "Batch: 235, iteration took 0.4583, Loss: 5.872502326965332\n",
            "Batch: 236, iteration took 0.4652, Loss: 5.882727146148682\n",
            "Batch: 237, iteration took 0.4575, Loss: 5.868618965148926\n",
            "Batch: 238, iteration took 0.4459, Loss: 5.80276346206665\n",
            "Batch: 239, iteration took 0.4601, Loss: 5.9231061935424805\n",
            "Batch: 240, iteration took 0.4487, Loss: 5.823772430419922\n",
            "Batch: 241, iteration took 0.4695, Loss: 5.857492446899414\n",
            "Batch: 242, iteration took 0.4690, Loss: 5.82680606842041\n",
            "Batch: 243, iteration took 0.4504, Loss: 5.816514015197754\n",
            "Batch: 244, iteration took 0.4922, Loss: 5.781238555908203\n",
            "Batch: 245, iteration took 0.4761, Loss: 5.726387023925781\n",
            "Batch: 246, iteration took 0.4744, Loss: 5.754186630249023\n",
            "Batch: 247, iteration took 0.4549, Loss: 5.750347137451172\n",
            "Batch: 248, iteration took 0.4708, Loss: 5.7877326011657715\n",
            "Batch: 249, iteration took 0.4814, Loss: 5.767152309417725\n",
            "Batch: 250, iteration took 0.4881, Loss: 5.765270233154297\n",
            "Batch: 251, iteration took 0.4741, Loss: 5.704673767089844\n",
            "Batch: 252, iteration took 0.4516, Loss: 5.733165740966797\n",
            "Batch: 253, iteration took 0.4795, Loss: 5.6800336837768555\n",
            "Batch: 254, iteration took 0.4644, Loss: 5.768303871154785\n",
            "Batch: 255, iteration took 0.4735, Loss: 5.673127174377441\n",
            "Batch: 256, iteration took 0.4569, Loss: 5.747391700744629\n",
            "Batch: 257, iteration took 0.4842, Loss: 5.653273105621338\n",
            "Batch: 258, iteration took 0.4782, Loss: 5.668449401855469\n",
            "Batch: 259, iteration took 0.4783, Loss: 5.667206287384033\n",
            "Batch: 260, iteration took 0.4635, Loss: 5.622749328613281\n",
            "Batch: 261, iteration took 0.4739, Loss: 5.581884384155273\n",
            "Batch: 262, iteration took 0.4789, Loss: 5.638625144958496\n",
            "Batch: 263, iteration took 0.4736, Loss: 5.627363681793213\n",
            "Batch: 264, iteration took 0.4652, Loss: 5.6363911628723145\n",
            "Batch: 265, iteration took 0.4586, Loss: 5.638021945953369\n",
            "Batch: 266, iteration took 0.4667, Loss: 5.62870979309082\n",
            "Batch: 267, iteration took 0.4669, Loss: 5.685844421386719\n",
            "Batch: 268, iteration took 0.4797, Loss: 5.571427345275879\n",
            "Batch: 269, iteration took 0.4500, Loss: 5.647500038146973\n",
            "Batch: 270, iteration took 0.4750, Loss: 5.600070953369141\n",
            "Batch: 271, iteration took 0.4653, Loss: 5.560766220092773\n",
            "Batch: 272, iteration took 0.4743, Loss: 5.553203582763672\n",
            "Batch: 273, iteration took 0.4546, Loss: 5.533216953277588\n",
            "Batch: 274, iteration took 0.4601, Loss: 5.559404373168945\n",
            "Batch: 275, iteration took 0.4798, Loss: 5.533158302307129\n",
            "Batch: 276, iteration took 0.4775, Loss: 5.538661479949951\n",
            "Batch: 277, iteration took 0.4741, Loss: 5.47551965713501\n",
            "Batch: 278, iteration took 0.4506, Loss: 5.546314239501953\n",
            "Batch: 279, iteration took 0.4773, Loss: 5.47648811340332\n",
            "Batch: 280, iteration took 0.4569, Loss: 5.45936393737793\n",
            "Batch: 281, iteration took 0.4680, Loss: 5.531113624572754\n",
            "Batch: 282, iteration took 0.4775, Loss: 5.474941253662109\n",
            "Batch: 283, iteration took 0.4807, Loss: 5.524250030517578\n",
            "Batch: 284, iteration took 0.4650, Loss: 5.493346214294434\n",
            "Batch: 285, iteration took 0.4822, Loss: 5.387170314788818\n",
            "Batch: 286, iteration took 0.4857, Loss: 5.465115547180176\n",
            "Batch: 287, iteration took 0.4532, Loss: 5.42313289642334\n",
            "Batch: 288, iteration took 0.4800, Loss: 5.462989807128906\n",
            "Batch: 289, iteration took 0.4568, Loss: 5.465574264526367\n",
            "Batch: 290, iteration took 0.4631, Loss: 5.530012130737305\n",
            "Batch: 291, iteration took 0.4894, Loss: 5.425338268280029\n",
            "Batch: 292, iteration took 0.4900, Loss: 5.362386703491211\n",
            "Batch: 293, iteration took 0.4759, Loss: 5.474829196929932\n",
            "Batch: 294, iteration took 0.4671, Loss: 5.463242053985596\n",
            "Batch: 295, iteration took 0.4785, Loss: 5.403040409088135\n",
            "Batch: 296, iteration took 0.4800, Loss: 5.406678199768066\n",
            "Batch: 297, iteration took 0.4732, Loss: 5.349734306335449\n",
            "Batch: 298, iteration took 0.4664, Loss: 5.4463043212890625\n",
            "Batch: 299, iteration took 0.4528, Loss: 5.398398399353027\n",
            "Batch: 300, iteration took 0.4881, Loss: 5.32077693939209\n",
            "Batch: 301, iteration took 0.4727, Loss: 5.419753074645996\n",
            "Batch: 302, iteration took 0.4726, Loss: 5.40364933013916\n",
            "Batch: 303, iteration took 0.4508, Loss: 5.346057891845703\n",
            "Batch: 304, iteration took 0.4742, Loss: 5.367413520812988\n",
            "Batch: 305, iteration took 0.4831, Loss: 5.349778175354004\n",
            "Batch: 306, iteration took 0.4849, Loss: 5.362218379974365\n",
            "Batch: 307, iteration took 0.4804, Loss: 5.3307695388793945\n",
            "Batch: 308, iteration took 0.4828, Loss: 5.271450996398926\n",
            "Batch: 309, iteration took 0.4532, Loss: 5.298232555389404\n",
            "Batch: 310, iteration took 0.4652, Loss: 5.241650581359863\n",
            "Batch: 311, iteration took 0.4613, Loss: 5.353234767913818\n",
            "Batch: 312, iteration took 0.4774, Loss: 5.3533830642700195\n",
            "Batch: 313, iteration took 0.4829, Loss: 5.317136764526367\n",
            "Batch: 314, iteration took 0.4878, Loss: 5.332890510559082\n",
            "Batch: 315, iteration took 0.4723, Loss: 5.340144634246826\n",
            "Batch: 316, iteration took 0.4614, Loss: 5.336146354675293\n",
            "Batch: 317, iteration took 0.4662, Loss: 5.237224102020264\n",
            "Batch: 318, iteration took 0.4546, Loss: 5.3915510177612305\n",
            "Batch: 319, iteration took 0.4578, Loss: 5.197185516357422\n",
            "Batch: 320, iteration took 0.4566, Loss: 5.341570854187012\n",
            "Batch: 321, iteration took 0.4604, Loss: 5.2864274978637695\n",
            "Batch: 322, iteration took 0.4734, Loss: 5.257600784301758\n",
            "Batch: 323, iteration took 0.4775, Loss: 5.2876386642456055\n",
            "Batch: 324, iteration took 0.4803, Loss: 5.237360000610352\n",
            "Batch: 325, iteration took 0.4759, Loss: 5.2068610191345215\n",
            "Batch: 326, iteration took 0.4906, Loss: 5.233399391174316\n",
            "Batch: 327, iteration took 0.4780, Loss: 5.257777690887451\n",
            "Batch: 328, iteration took 0.4715, Loss: 5.248271942138672\n",
            "Batch: 329, iteration took 0.4544, Loss: 5.282925605773926\n",
            "Batch: 330, iteration took 0.4656, Loss: 5.262472152709961\n",
            "Batch: 331, iteration took 0.4593, Loss: 5.221067428588867\n",
            "Batch: 332, iteration took 0.4521, Loss: 5.252459526062012\n",
            "Batch: 333, iteration took 0.4591, Loss: 5.21883487701416\n",
            "Batch: 334, iteration took 0.4780, Loss: 5.21510648727417\n",
            "Batch: 335, iteration took 0.4625, Loss: 5.217461585998535\n",
            "Batch: 336, iteration took 0.4620, Loss: 5.199370384216309\n",
            "Batch: 337, iteration took 0.4711, Loss: 5.163659572601318\n",
            "Batch: 338, iteration took 0.4763, Loss: 5.219168186187744\n",
            "Batch: 339, iteration took 0.4853, Loss: 5.134987831115723\n",
            "Batch: 340, iteration took 0.4695, Loss: 5.215954780578613\n",
            "Batch: 341, iteration took 0.4511, Loss: 5.165481090545654\n",
            "Batch: 342, iteration took 0.4680, Loss: 5.209072113037109\n",
            "Batch: 343, iteration took 0.4784, Loss: 5.179915428161621\n",
            "Batch: 344, iteration took 0.4776, Loss: 5.156225204467773\n",
            "Batch: 345, iteration took 0.4492, Loss: 5.176517486572266\n",
            "Batch: 346, iteration took 0.4684, Loss: 5.245429039001465\n",
            "Batch: 347, iteration took 0.4619, Loss: 5.194609642028809\n",
            "Batch: 348, iteration took 0.4653, Loss: 5.125388145446777\n",
            "Batch: 349, iteration took 0.4803, Loss: 5.183600425720215\n",
            "Batch: 350, iteration took 0.4480, Loss: 5.137349605560303\n",
            "Batch: 351, iteration took 0.4823, Loss: 5.207558631896973\n",
            "Batch: 352, iteration took 0.4715, Loss: 5.192600250244141\n",
            "Batch: 353, iteration took 0.4757, Loss: 5.206148147583008\n",
            "Batch: 354, iteration took 0.4694, Loss: 5.165359020233154\n",
            "Batch: 355, iteration took 0.4784, Loss: 5.170958995819092\n",
            "Batch: 356, iteration took 0.4757, Loss: 5.160789489746094\n",
            "Batch: 357, iteration took 0.4900, Loss: 5.196063995361328\n",
            "Batch: 358, iteration took 0.4639, Loss: 5.126232624053955\n",
            "Batch: 359, iteration took 0.4568, Loss: 5.139586925506592\n",
            "Batch: 360, iteration took 0.4640, Loss: 5.10577392578125\n",
            "Batch: 361, iteration took 0.4557, Loss: 5.146706581115723\n",
            "Batch: 362, iteration took 0.4699, Loss: 5.146836280822754\n",
            "Batch: 363, iteration took 0.4587, Loss: 5.1964263916015625\n",
            "Batch: 364, iteration took 0.4702, Loss: 5.100940704345703\n",
            "Batch: 365, iteration took 0.4853, Loss: 5.057571887969971\n",
            "Batch: 366, iteration took 0.4821, Loss: 5.111653804779053\n",
            "Batch: 367, iteration took 0.4761, Loss: 5.0977373123168945\n",
            "Batch: 368, iteration took 0.4878, Loss: 5.097904205322266\n",
            "Batch: 369, iteration took 0.4849, Loss: 5.044503211975098\n",
            "Batch: 370, iteration took 0.4798, Loss: 5.177387714385986\n",
            "Batch: 371, iteration took 0.4725, Loss: 5.147916793823242\n",
            "Batch: 372, iteration took 0.4629, Loss: 5.094186782836914\n",
            "Batch: 373, iteration took 0.4738, Loss: 5.153277397155762\n",
            "Batch: 374, iteration took 0.4579, Loss: 5.0076680183410645\n",
            "Batch: 375, iteration took 0.4808, Loss: 5.125209808349609\n",
            "Batch: 376, iteration took 0.4850, Loss: 5.200639247894287\n",
            "Batch: 377, iteration took 0.4784, Loss: 5.068853378295898\n",
            "Batch: 378, iteration took 0.4593, Loss: 5.128889083862305\n",
            "Batch: 379, iteration took 0.4591, Loss: 5.055980205535889\n",
            "Batch: 380, iteration took 0.4963, Loss: 5.085421562194824\n",
            "Batch: 381, iteration took 0.4676, Loss: 4.980072975158691\n",
            "Batch: 382, iteration took 0.4757, Loss: 5.034868240356445\n",
            "Batch: 383, iteration took 0.4654, Loss: 5.099921703338623\n",
            "Batch: 384, iteration took 0.4697, Loss: 5.152507781982422\n",
            "Batch: 385, iteration took 0.4633, Loss: 5.037383079528809\n",
            "Batch: 386, iteration took 0.4574, Loss: 5.047582149505615\n",
            "Batch: 387, iteration took 0.4647, Loss: 5.075014114379883\n",
            "Batch: 388, iteration took 0.4559, Loss: 5.058279037475586\n",
            "Batch: 389, iteration took 0.4922, Loss: 5.07203483581543\n",
            "Batch: 390, iteration took 0.4653, Loss: 5.053985595703125\n",
            "Batch: 391, iteration took 0.4636, Loss: 5.066798210144043\n",
            "Batch: 392, iteration took 0.4512, Loss: 5.060555934906006\n",
            "Batch: 393, iteration took 0.4835, Loss: 5.031156539916992\n",
            "Batch: 394, iteration took 0.4550, Loss: 5.048036575317383\n",
            "Batch: 395, iteration took 0.4549, Loss: 5.022181034088135\n",
            "Batch: 396, iteration took 0.4669, Loss: 5.031411647796631\n",
            "Batch: 397, iteration took 0.4769, Loss: 5.04133939743042\n",
            "Batch: 398, iteration took 0.4697, Loss: 5.0273051261901855\n",
            "avg train bits per pixel 5.4767\n",
            "Batch: 399, iteration took 1.1229, Loss: 5.020260334014893\n",
            "Batch: 400, iteration took 0.4500, Loss: 5.040377140045166\n",
            "Batch: 401, iteration took 0.4761, Loss: 4.969619274139404\n",
            "Batch: 402, iteration took 0.4700, Loss: 5.011662483215332\n",
            "Batch: 403, iteration took 0.4578, Loss: 4.946369647979736\n",
            "Batch: 404, iteration took 0.4576, Loss: 5.073533535003662\n",
            "Batch: 405, iteration took 0.4870, Loss: 5.028078079223633\n",
            "Batch: 406, iteration took 0.4984, Loss: 4.937809944152832\n",
            "Batch: 407, iteration took 0.4767, Loss: 5.072467803955078\n",
            "Batch: 408, iteration took 0.4593, Loss: 4.97188138961792\n",
            "Batch: 409, iteration took 0.4861, Loss: 5.011787414550781\n",
            "Batch: 410, iteration took 0.4772, Loss: 4.983611106872559\n",
            "Batch: 411, iteration took 0.4676, Loss: 5.038345813751221\n",
            "Batch: 412, iteration took 0.4696, Loss: 4.9907121658325195\n",
            "Batch: 413, iteration took 0.4742, Loss: 5.014196872711182\n",
            "Batch: 414, iteration took 0.4794, Loss: 5.029886245727539\n",
            "Batch: 415, iteration took 0.4687, Loss: 4.97529411315918\n",
            "Batch: 416, iteration took 0.4889, Loss: 4.9624738693237305\n",
            "Batch: 417, iteration took 0.4640, Loss: 5.0390825271606445\n",
            "Batch: 418, iteration took 0.4521, Loss: 5.082888126373291\n",
            "Batch: 419, iteration took 0.4549, Loss: 5.01253604888916\n",
            "Batch: 420, iteration took 0.4678, Loss: 4.906601428985596\n",
            "Batch: 421, iteration took 0.4785, Loss: 5.00317907333374\n",
            "Batch: 422, iteration took 0.4610, Loss: 4.9908294677734375\n",
            "Batch: 423, iteration took 0.4521, Loss: 5.052173614501953\n",
            "Batch: 424, iteration took 0.4736, Loss: 4.9142913818359375\n",
            "Batch: 425, iteration took 0.4657, Loss: 4.991526126861572\n",
            "Batch: 426, iteration took 0.4756, Loss: 4.989948272705078\n",
            "Batch: 427, iteration took 0.4882, Loss: 5.018401145935059\n",
            "Batch: 428, iteration took 0.4677, Loss: 4.984627723693848\n",
            "Batch: 429, iteration took 0.4613, Loss: 4.9716339111328125\n",
            "Batch: 430, iteration took 0.4786, Loss: 5.024377822875977\n",
            "Batch: 431, iteration took 0.4893, Loss: 4.916922569274902\n",
            "Batch: 432, iteration took 0.4716, Loss: 4.924868583679199\n",
            "Batch: 433, iteration took 0.4805, Loss: 5.070717811584473\n",
            "Batch: 434, iteration took 0.4678, Loss: 4.874166488647461\n",
            "Batch: 435, iteration took 0.4535, Loss: 4.998591423034668\n",
            "Batch: 436, iteration took 0.4510, Loss: 4.944276809692383\n",
            "Batch: 437, iteration took 0.4557, Loss: 5.027723789215088\n",
            "Batch: 438, iteration took 0.4505, Loss: 4.987195014953613\n",
            "Batch: 439, iteration took 0.4697, Loss: 4.927499771118164\n",
            "Batch: 440, iteration took 0.4755, Loss: 4.986739635467529\n",
            "Batch: 441, iteration took 0.4536, Loss: 4.942833423614502\n",
            "Batch: 442, iteration took 0.4694, Loss: 4.889263153076172\n",
            "Batch: 443, iteration took 0.4614, Loss: 5.006707668304443\n",
            "Batch: 444, iteration took 0.4569, Loss: 4.869860649108887\n",
            "Batch: 445, iteration took 0.4574, Loss: 5.009342670440674\n",
            "Batch: 446, iteration took 0.4789, Loss: 4.867133140563965\n",
            "Batch: 447, iteration took 0.4577, Loss: 4.991947174072266\n",
            "Batch: 448, iteration took 0.4529, Loss: 4.935245037078857\n",
            "Batch: 449, iteration took 0.4809, Loss: 5.039987564086914\n",
            "Batch: 450, iteration took 0.4783, Loss: 4.831363201141357\n",
            "Batch: 451, iteration took 0.4867, Loss: 4.947172164916992\n",
            "Batch: 452, iteration took 0.4577, Loss: 4.9708251953125\n",
            "Batch: 453, iteration took 0.4671, Loss: 4.9886603355407715\n",
            "Batch: 454, iteration took 0.4704, Loss: 4.9068498611450195\n",
            "Batch: 455, iteration took 0.4522, Loss: 4.92105770111084\n",
            "Batch: 456, iteration took 0.4528, Loss: 4.9419755935668945\n",
            "Batch: 457, iteration took 0.4509, Loss: 4.875304698944092\n",
            "Batch: 458, iteration took 0.4850, Loss: 4.976250648498535\n",
            "Batch: 459, iteration took 0.4684, Loss: 4.8879547119140625\n",
            "Batch: 460, iteration took 0.4658, Loss: 4.952996253967285\n",
            "Batch: 461, iteration took 0.4532, Loss: 4.950097560882568\n",
            "Batch: 462, iteration took 0.4620, Loss: 4.893352508544922\n",
            "Batch: 463, iteration took 0.4522, Loss: 4.915831089019775\n",
            "Batch: 464, iteration took 0.4794, Loss: 4.910390853881836\n",
            "Batch: 465, iteration took 0.4723, Loss: 4.921179294586182\n",
            "Batch: 466, iteration took 0.4659, Loss: 5.019447326660156\n",
            "Batch: 467, iteration took 0.4501, Loss: 4.942313194274902\n",
            "Batch: 468, iteration took 0.4702, Loss: 4.979181289672852\n",
            "Batch: 469, iteration took 0.4671, Loss: 4.958191871643066\n",
            "Batch: 470, iteration took 0.4587, Loss: 4.9458136558532715\n",
            "Batch: 471, iteration took 0.4547, Loss: 4.8893537521362305\n",
            "Batch: 472, iteration took 0.4511, Loss: 4.848980903625488\n",
            "Batch: 473, iteration took 0.4644, Loss: 4.889030456542969\n",
            "Batch: 474, iteration took 0.4535, Loss: 4.965953826904297\n",
            "Batch: 475, iteration took 0.4584, Loss: 4.952622890472412\n",
            "Batch: 476, iteration took 0.4503, Loss: 4.947465896606445\n",
            "Batch: 477, iteration took 0.4486, Loss: 4.868306636810303\n",
            "Batch: 478, iteration took 0.4536, Loss: 4.996161460876465\n",
            "Batch: 479, iteration took 0.4497, Loss: 4.927072525024414\n",
            "Batch: 480, iteration took 0.4747, Loss: 4.9190673828125\n",
            "Batch: 481, iteration took 0.4641, Loss: 4.9314751625061035\n",
            "Batch: 482, iteration took 0.4582, Loss: 4.972775459289551\n",
            "Batch: 483, iteration took 0.4496, Loss: 4.968569755554199\n",
            "Batch: 484, iteration took 0.4559, Loss: 4.856204032897949\n",
            "Batch: 485, iteration took 0.4510, Loss: 4.987432479858398\n",
            "Batch: 486, iteration took 0.4606, Loss: 5.0453691482543945\n",
            "Batch: 487, iteration took 0.4519, Loss: 4.9107513427734375\n",
            "Batch: 488, iteration took 0.4545, Loss: 4.959237098693848\n",
            "Batch: 489, iteration took 0.4541, Loss: 4.929970741271973\n",
            "Batch: 490, iteration took 0.4523, Loss: 4.998677730560303\n",
            "Batch: 491, iteration took 0.4653, Loss: 4.8692827224731445\n",
            "Batch: 492, iteration took 0.4576, Loss: 4.891695976257324\n",
            "Batch: 493, iteration took 0.4509, Loss: 4.919602394104004\n",
            "Batch: 494, iteration took 0.4610, Loss: 4.90388822555542\n",
            "Batch: 495, iteration took 0.4621, Loss: 4.860107421875\n",
            "Batch: 496, iteration took 0.4629, Loss: 4.878633499145508\n",
            "Batch: 497, iteration took 0.4544, Loss: 4.905118942260742\n",
            "Batch: 498, iteration took 0.4719, Loss: 4.84319543838501\n",
            "Batch: 499, iteration took 0.4565, Loss: 4.746328353881836\n",
            "Batch: 500, iteration took 0.4479, Loss: 4.905836582183838\n",
            "Batch: 501, iteration took 0.4635, Loss: 4.9325480461120605\n",
            "Batch: 502, iteration took 0.4510, Loss: 4.814150333404541\n",
            "Batch: 503, iteration took 0.4576, Loss: 4.869131088256836\n",
            "Batch: 504, iteration took 0.4699, Loss: 4.885914325714111\n",
            "Batch: 505, iteration took 0.4530, Loss: 4.851588726043701\n",
            "Batch: 506, iteration took 0.4602, Loss: 4.882358074188232\n",
            "Batch: 507, iteration took 0.4422, Loss: 4.9477081298828125\n",
            "Batch: 508, iteration took 0.4733, Loss: 4.842229843139648\n",
            "Batch: 509, iteration took 0.4514, Loss: 4.843071937561035\n",
            "Batch: 510, iteration took 0.4797, Loss: 4.893672943115234\n",
            "Batch: 511, iteration took 0.4500, Loss: 4.86275577545166\n",
            "Batch: 512, iteration took 0.4581, Loss: 4.8285722732543945\n",
            "Batch: 513, iteration took 0.4531, Loss: 4.931361198425293\n",
            "Batch: 514, iteration took 0.4561, Loss: 4.82851505279541\n",
            "Batch: 515, iteration took 0.4686, Loss: 4.945740222930908\n",
            "Batch: 516, iteration took 0.4492, Loss: 4.837689399719238\n",
            "Batch: 517, iteration took 0.4798, Loss: 4.95052433013916\n",
            "Batch: 518, iteration took 0.4775, Loss: 4.797244071960449\n",
            "Batch: 519, iteration took 0.4910, Loss: 4.945347785949707\n",
            "Batch: 520, iteration took 0.4794, Loss: 4.806276798248291\n",
            "Batch: 521, iteration took 0.4638, Loss: 4.996265411376953\n",
            "Batch: 522, iteration took 0.4457, Loss: 4.986478805541992\n",
            "Batch: 523, iteration took 0.4576, Loss: 4.881685733795166\n",
            "Batch: 524, iteration took 0.4711, Loss: 5.005053520202637\n",
            "Batch: 525, iteration took 0.4719, Loss: 4.928262710571289\n",
            "Batch: 526, iteration took 0.4669, Loss: 4.888343811035156\n",
            "Batch: 527, iteration took 0.4467, Loss: 4.948601722717285\n",
            "Batch: 528, iteration took 0.4522, Loss: 4.835914611816406\n",
            "Batch: 529, iteration took 0.4483, Loss: 4.7618727684021\n",
            "Batch: 530, iteration took 0.4523, Loss: 4.766628742218018\n",
            "Batch: 531, iteration took 0.4726, Loss: 4.921982288360596\n",
            "Batch: 532, iteration took 0.4694, Loss: 4.815304756164551\n",
            "Batch: 533, iteration took 0.4717, Loss: 4.804893970489502\n",
            "Batch: 534, iteration took 0.4627, Loss: 4.812049865722656\n",
            "Batch: 535, iteration took 0.4562, Loss: 4.812627792358398\n",
            "Batch: 536, iteration took 0.4455, Loss: 4.876349925994873\n",
            "Batch: 537, iteration took 0.4628, Loss: 4.845046520233154\n",
            "Batch: 538, iteration took 0.4534, Loss: 4.90046501159668\n",
            "Batch: 539, iteration took 0.4520, Loss: 4.875888824462891\n",
            "Batch: 540, iteration took 0.4628, Loss: 4.853072643280029\n",
            "Batch: 541, iteration took 0.4813, Loss: 4.86555290222168\n",
            "Batch: 542, iteration took 0.4745, Loss: 4.732657432556152\n",
            "Batch: 543, iteration took 0.4632, Loss: 4.934684753417969\n",
            "Batch: 544, iteration took 0.4547, Loss: 4.7611613273620605\n",
            "Batch: 545, iteration took 0.4596, Loss: 4.915657043457031\n",
            "Batch: 546, iteration took 0.4903, Loss: 4.782877445220947\n",
            "Batch: 547, iteration took 0.4648, Loss: 5.033016204833984\n",
            "Batch: 548, iteration took 0.4585, Loss: 4.9747233390808105\n",
            "Batch: 549, iteration took 0.4704, Loss: 4.974059104919434\n",
            "Batch: 550, iteration took 0.4672, Loss: 4.996153354644775\n",
            "Batch: 551, iteration took 0.4554, Loss: 4.884589195251465\n",
            "Batch: 552, iteration took 0.4641, Loss: 4.903244972229004\n",
            "Batch: 553, iteration took 0.4637, Loss: 4.838528156280518\n",
            "Batch: 554, iteration took 0.4523, Loss: 4.837985992431641\n",
            "Batch: 555, iteration took 0.4681, Loss: 4.861977577209473\n",
            "Batch: 556, iteration took 0.4837, Loss: 4.86184549331665\n",
            "Batch: 557, iteration took 0.4584, Loss: 4.923994064331055\n",
            "Batch: 558, iteration took 0.4829, Loss: 4.807240009307861\n",
            "Batch: 559, iteration took 0.4554, Loss: 4.841541290283203\n",
            "Batch: 560, iteration took 0.4625, Loss: 4.911550998687744\n",
            "Batch: 561, iteration took 0.4585, Loss: 4.836399078369141\n",
            "Batch: 562, iteration took 0.4769, Loss: 4.894244194030762\n",
            "Batch: 563, iteration took 0.4578, Loss: 4.808910369873047\n",
            "Batch: 564, iteration took 0.4611, Loss: 4.776642799377441\n",
            "Batch: 565, iteration took 0.4510, Loss: 4.942538261413574\n",
            "Batch: 566, iteration took 0.4541, Loss: 4.915895462036133\n",
            "Batch: 567, iteration took 0.4528, Loss: 4.835939407348633\n",
            "Batch: 568, iteration took 0.4533, Loss: 4.799836158752441\n",
            "Batch: 569, iteration took 0.4723, Loss: 4.777667045593262\n",
            "Batch: 570, iteration took 0.4606, Loss: 4.805873394012451\n",
            "Batch: 571, iteration took 0.4630, Loss: 4.788193702697754\n",
            "Batch: 572, iteration took 0.4902, Loss: 4.874923229217529\n",
            "Batch: 573, iteration took 0.4612, Loss: 4.892457962036133\n",
            "Batch: 574, iteration took 0.4494, Loss: 4.7800679206848145\n",
            "Batch: 575, iteration took 0.4515, Loss: 4.782406806945801\n",
            "Batch: 576, iteration took 0.4570, Loss: 4.900458335876465\n",
            "Batch: 577, iteration took 0.4499, Loss: 4.899415969848633\n",
            "Batch: 578, iteration took 0.4688, Loss: 4.950551509857178\n",
            "Batch: 579, iteration took 0.4546, Loss: 4.7832183837890625\n",
            "Batch: 580, iteration took 0.4548, Loss: 4.812344551086426\n",
            "Batch: 581, iteration took 0.4847, Loss: 4.838995933532715\n",
            "Batch: 582, iteration took 0.4532, Loss: 4.811463356018066\n",
            "Batch: 583, iteration took 0.4808, Loss: 4.858294486999512\n",
            "Batch: 584, iteration took 0.4493, Loss: 4.814561367034912\n",
            "Batch: 585, iteration took 0.4566, Loss: 4.81859016418457\n",
            "Batch: 586, iteration took 0.4698, Loss: 4.885401248931885\n",
            "Batch: 587, iteration took 0.4526, Loss: 4.755388259887695\n",
            "Batch: 588, iteration took 0.4511, Loss: 4.767294883728027\n",
            "Batch: 589, iteration took 0.4570, Loss: 4.787352085113525\n",
            "Batch: 590, iteration took 0.4563, Loss: 4.793944358825684\n",
            "Batch: 591, iteration took 0.4633, Loss: 4.735555648803711\n",
            "Batch: 592, iteration took 0.4646, Loss: 4.776288986206055\n",
            "Batch: 593, iteration took 0.4477, Loss: 4.695093631744385\n",
            "Batch: 594, iteration took 0.4472, Loss: 4.799002647399902\n",
            "Batch: 595, iteration took 0.4532, Loss: 4.84550666809082\n",
            "Batch: 596, iteration took 0.4574, Loss: 4.784849166870117\n",
            "Batch: 597, iteration took 0.4560, Loss: 4.891473293304443\n",
            "Batch: 598, iteration took 0.4459, Loss: 4.919874668121338\n",
            "avg train bits per pixel 4.9061\n",
            "Batch: 599, iteration took 1.2087, Loss: 4.741726398468018\n",
            "Batch: 600, iteration took 0.4533, Loss: 4.75637674331665\n",
            "Batch: 601, iteration took 0.4566, Loss: 4.860413074493408\n",
            "Batch: 602, iteration took 0.4591, Loss: 4.721364498138428\n",
            "Batch: 603, iteration took 0.4505, Loss: 4.796331405639648\n",
            "Batch: 604, iteration took 0.4666, Loss: 4.6921515464782715\n",
            "Batch: 605, iteration took 0.4535, Loss: 4.7389631271362305\n",
            "Batch: 606, iteration took 0.4758, Loss: 4.8524556159973145\n",
            "Batch: 607, iteration took 0.4527, Loss: 4.820940017700195\n",
            "Batch: 608, iteration took 0.4616, Loss: 4.687964916229248\n",
            "Batch: 609, iteration took 0.4677, Loss: 4.810772895812988\n",
            "Batch: 610, iteration took 0.4561, Loss: 4.736161231994629\n",
            "Batch: 611, iteration took 0.4691, Loss: 4.830512523651123\n",
            "Batch: 612, iteration took 0.4809, Loss: 4.823477745056152\n",
            "Batch: 613, iteration took 0.4507, Loss: 4.735029697418213\n",
            "Batch: 614, iteration took 0.4570, Loss: 4.6966447830200195\n",
            "Batch: 615, iteration took 0.4605, Loss: 4.684664726257324\n",
            "Batch: 616, iteration took 0.4783, Loss: 4.667264938354492\n",
            "Batch: 617, iteration took 0.4692, Loss: 4.740533828735352\n",
            "Batch: 618, iteration took 0.4691, Loss: 4.657713890075684\n",
            "Batch: 619, iteration took 0.4951, Loss: 4.804064750671387\n",
            "Batch: 620, iteration took 0.4723, Loss: 4.724809646606445\n",
            "Batch: 621, iteration took 0.4595, Loss: 4.678147315979004\n",
            "Batch: 622, iteration took 0.4488, Loss: 4.743782997131348\n",
            "Batch: 623, iteration took 0.4929, Loss: 4.797708511352539\n",
            "Batch: 624, iteration took 0.4696, Loss: 4.774203777313232\n",
            "Batch: 625, iteration took 0.4790, Loss: 4.663475036621094\n",
            "Batch: 626, iteration took 0.4597, Loss: 4.673916339874268\n",
            "Batch: 627, iteration took 0.4522, Loss: 4.818331241607666\n",
            "Batch: 628, iteration took 0.4635, Loss: 4.773780822753906\n",
            "Batch: 629, iteration took 0.4497, Loss: 4.685296058654785\n",
            "Batch: 630, iteration took 0.4521, Loss: 4.724337577819824\n",
            "Batch: 631, iteration took 0.4514, Loss: 4.765237808227539\n",
            "Batch: 632, iteration took 0.4588, Loss: 4.769950866699219\n",
            "Batch: 633, iteration took 0.4764, Loss: 4.735690593719482\n",
            "Batch: 634, iteration took 0.4550, Loss: 4.735478401184082\n",
            "Batch: 635, iteration took 0.4455, Loss: 4.746806621551514\n",
            "Batch: 636, iteration took 0.4825, Loss: 4.742595672607422\n",
            "Batch: 637, iteration took 0.4842, Loss: 4.7967681884765625\n",
            "Batch: 638, iteration took 0.4626, Loss: 4.778207778930664\n",
            "Batch: 639, iteration took 0.4825, Loss: 4.7601118087768555\n",
            "Batch: 640, iteration took 0.4783, Loss: 4.7701215744018555\n",
            "Batch: 641, iteration took 0.4510, Loss: 4.635274887084961\n",
            "Batch: 642, iteration took 0.4521, Loss: 4.7351179122924805\n",
            "Batch: 643, iteration took 0.4699, Loss: 4.833248138427734\n",
            "Batch: 644, iteration took 0.4493, Loss: 4.705604553222656\n",
            "Batch: 645, iteration took 0.4632, Loss: 4.806981086730957\n",
            "Batch: 646, iteration took 0.4491, Loss: 4.786808967590332\n",
            "Batch: 647, iteration took 0.4747, Loss: 4.733270168304443\n",
            "Batch: 648, iteration took 0.4810, Loss: 4.671940326690674\n",
            "Batch: 649, iteration took 0.4779, Loss: 4.804930686950684\n",
            "Batch: 650, iteration took 0.4513, Loss: 4.759391784667969\n",
            "Batch: 651, iteration took 0.4493, Loss: 4.819138050079346\n",
            "Batch: 652, iteration took 0.4881, Loss: 4.7244672775268555\n",
            "Batch: 653, iteration took 0.4591, Loss: 4.869711875915527\n",
            "Batch: 654, iteration took 0.4522, Loss: 4.818633079528809\n",
            "Batch: 655, iteration took 0.4507, Loss: 4.7672529220581055\n",
            "Batch: 656, iteration took 0.4672, Loss: 4.731078147888184\n",
            "Batch: 657, iteration took 0.4567, Loss: 4.764395713806152\n",
            "Batch: 658, iteration took 0.4625, Loss: 4.716273784637451\n",
            "Batch: 659, iteration took 0.4513, Loss: 4.801359176635742\n",
            "Batch: 660, iteration took 0.4534, Loss: 4.786285400390625\n",
            "Batch: 661, iteration took 0.4540, Loss: 4.669138431549072\n",
            "Batch: 662, iteration took 0.4555, Loss: 4.657716274261475\n",
            "Batch: 663, iteration took 0.4702, Loss: 4.768960952758789\n",
            "Batch: 664, iteration took 0.4521, Loss: 4.728171348571777\n",
            "Batch: 665, iteration took 0.4488, Loss: 4.690028190612793\n",
            "Batch: 666, iteration took 0.4535, Loss: 4.683244228363037\n",
            "Batch: 667, iteration took 0.4528, Loss: 4.7431135177612305\n",
            "Batch: 668, iteration took 0.4645, Loss: 4.771531105041504\n",
            "Batch: 669, iteration took 0.4469, Loss: 4.755673408508301\n",
            "Batch: 670, iteration took 0.4487, Loss: 4.823655128479004\n",
            "Batch: 671, iteration took 0.4785, Loss: 4.716436386108398\n",
            "Batch: 672, iteration took 0.4729, Loss: 4.7293596267700195\n",
            "Batch: 673, iteration took 0.4640, Loss: 4.735334396362305\n",
            "Batch: 674, iteration took 0.4549, Loss: 4.678127765655518\n",
            "Batch: 675, iteration took 0.4848, Loss: 4.728265762329102\n",
            "Batch: 676, iteration took 0.4592, Loss: 4.689271450042725\n",
            "Batch: 677, iteration took 0.4509, Loss: 4.691343784332275\n",
            "Batch: 678, iteration took 0.4466, Loss: 4.664712905883789\n",
            "Batch: 679, iteration took 0.4555, Loss: 4.748355865478516\n",
            "Batch: 680, iteration took 0.4768, Loss: 4.702749252319336\n",
            "Batch: 681, iteration took 0.4648, Loss: 4.685949802398682\n",
            "Batch: 682, iteration took 0.4456, Loss: 4.750638008117676\n",
            "Batch: 683, iteration took 0.4566, Loss: 4.826082229614258\n",
            "Batch: 684, iteration took 0.4682, Loss: 4.688498497009277\n",
            "Batch: 685, iteration took 0.4878, Loss: 4.99949836730957\n",
            "Batch: 686, iteration took 0.4569, Loss: 4.875054836273193\n",
            "Batch: 687, iteration took 0.4488, Loss: 4.789806365966797\n",
            "Batch: 688, iteration took 0.4733, Loss: 4.979143142700195\n",
            "Batch: 689, iteration took 0.4612, Loss: 4.914227485656738\n",
            "Batch: 690, iteration took 0.4569, Loss: 4.866476058959961\n",
            "Batch: 691, iteration took 0.4897, Loss: 4.955077171325684\n",
            "Batch: 692, iteration took 0.4631, Loss: 4.862411022186279\n",
            "Batch: 693, iteration took 0.4515, Loss: 4.856663703918457\n",
            "Batch: 694, iteration took 0.4585, Loss: 4.8320722579956055\n",
            "Batch: 695, iteration took 0.4553, Loss: 4.858831882476807\n",
            "Batch: 696, iteration took 0.4703, Loss: 4.793028831481934\n",
            "Batch: 697, iteration took 0.4768, Loss: 4.709833145141602\n",
            "Batch: 698, iteration took 0.4586, Loss: 4.756110668182373\n",
            "Batch: 699, iteration took 0.4543, Loss: 4.7575225830078125\n",
            "Batch: 700, iteration took 0.4693, Loss: 4.742421627044678\n",
            "Batch: 701, iteration took 0.4635, Loss: 4.750422954559326\n",
            "Batch: 702, iteration took 0.4558, Loss: 4.777247905731201\n",
            "Batch: 703, iteration took 0.4533, Loss: 4.754055023193359\n",
            "Batch: 704, iteration took 0.4504, Loss: 4.738626003265381\n",
            "Batch: 705, iteration took 0.4517, Loss: 4.814706802368164\n",
            "Batch: 706, iteration took 0.4474, Loss: 4.705384254455566\n",
            "Batch: 707, iteration took 0.4706, Loss: 4.69677734375\n",
            "Batch: 708, iteration took 0.4636, Loss: 4.734254837036133\n",
            "Batch: 709, iteration took 0.4680, Loss: 4.679755210876465\n",
            "Batch: 710, iteration took 0.4604, Loss: 4.843528747558594\n",
            "Batch: 711, iteration took 0.4736, Loss: 4.686450004577637\n",
            "Batch: 712, iteration took 0.4462, Loss: 4.670660972595215\n",
            "Batch: 713, iteration took 0.4561, Loss: 4.739606857299805\n",
            "Batch: 714, iteration took 0.4603, Loss: 4.662485122680664\n",
            "Batch: 715, iteration took 0.4456, Loss: 4.81400203704834\n",
            "Batch: 716, iteration took 0.4525, Loss: 4.671389579772949\n",
            "Batch: 717, iteration took 0.4739, Loss: 4.770560264587402\n",
            "Batch: 718, iteration took 0.4615, Loss: 4.722264289855957\n",
            "Batch: 719, iteration took 0.4762, Loss: 4.594707012176514\n",
            "Batch: 720, iteration took 0.4727, Loss: 4.705530166625977\n",
            "Batch: 721, iteration took 0.4689, Loss: 4.651803016662598\n",
            "Batch: 722, iteration took 0.4542, Loss: 4.757826805114746\n",
            "Batch: 723, iteration took 0.4703, Loss: 4.654894828796387\n",
            "Batch: 724, iteration took 0.4504, Loss: 4.798931121826172\n",
            "Batch: 725, iteration took 0.4692, Loss: 4.812216758728027\n",
            "Batch: 726, iteration took 0.4619, Loss: 4.638373374938965\n",
            "Batch: 727, iteration took 0.4635, Loss: 4.693873405456543\n",
            "Batch: 728, iteration took 0.4777, Loss: 4.756153106689453\n",
            "Batch: 729, iteration took 0.4859, Loss: 4.759358882904053\n",
            "Batch: 730, iteration took 0.4788, Loss: 4.790571212768555\n",
            "Batch: 731, iteration took 0.4573, Loss: 4.776893138885498\n",
            "Batch: 732, iteration took 0.4787, Loss: 4.747818946838379\n",
            "Batch: 733, iteration took 0.4658, Loss: 4.6095356941223145\n",
            "Batch: 734, iteration took 0.4719, Loss: 4.668168067932129\n",
            "Batch: 735, iteration took 0.4791, Loss: 4.697512626647949\n",
            "Batch: 736, iteration took 0.4697, Loss: 4.65923547744751\n",
            "Batch: 737, iteration took 0.4513, Loss: 4.66696310043335\n",
            "Batch: 738, iteration took 0.4793, Loss: 4.675498962402344\n",
            "Batch: 739, iteration took 0.4809, Loss: 4.785314559936523\n",
            "Batch: 740, iteration took 0.4503, Loss: 4.761711120605469\n",
            "Batch: 741, iteration took 0.4731, Loss: 4.639293670654297\n",
            "Batch: 742, iteration took 0.4505, Loss: 4.65076208114624\n",
            "Batch: 743, iteration took 0.4793, Loss: 4.668889045715332\n",
            "Batch: 744, iteration took 0.4500, Loss: 4.611215591430664\n",
            "Batch: 745, iteration took 0.4697, Loss: 4.709435939788818\n",
            "Batch: 746, iteration took 0.4550, Loss: 4.645221710205078\n",
            "Batch: 747, iteration took 0.4631, Loss: 4.835790634155273\n",
            "Batch: 748, iteration took 0.4536, Loss: 4.742874622344971\n",
            "Batch: 749, iteration took 0.4433, Loss: 4.755071640014648\n",
            "Batch: 750, iteration took 0.4625, Loss: 4.7956390380859375\n",
            "Batch: 751, iteration took 0.4515, Loss: 4.752552032470703\n",
            "Batch: 752, iteration took 0.4703, Loss: 4.7475481033325195\n",
            "Batch: 753, iteration took 0.4635, Loss: 4.7914509773254395\n",
            "Batch: 754, iteration took 0.4668, Loss: 4.788405418395996\n",
            "Batch: 755, iteration took 0.4721, Loss: 4.649563789367676\n",
            "Batch: 756, iteration took 0.4425, Loss: 4.747802734375\n",
            "Batch: 757, iteration took 0.4694, Loss: 4.70469331741333\n",
            "Batch: 758, iteration took 0.4689, Loss: 4.700713157653809\n",
            "Batch: 759, iteration took 0.4717, Loss: 4.617554664611816\n",
            "Batch: 760, iteration took 0.4511, Loss: 4.674572944641113\n",
            "Batch: 761, iteration took 0.4791, Loss: 4.640649318695068\n",
            "Batch: 762, iteration took 0.4674, Loss: 4.6288652420043945\n",
            "Batch: 763, iteration took 0.4750, Loss: 4.725217342376709\n",
            "Batch: 764, iteration took 0.4758, Loss: 4.6129655838012695\n",
            "Batch: 765, iteration took 0.4536, Loss: 4.647076606750488\n",
            "Batch: 766, iteration took 0.4786, Loss: 4.725589752197266\n",
            "Batch: 767, iteration took 0.4556, Loss: 4.661316871643066\n",
            "Batch: 768, iteration took 0.4696, Loss: 4.701473712921143\n",
            "Batch: 769, iteration took 0.4568, Loss: 4.619963645935059\n",
            "Batch: 770, iteration took 0.4913, Loss: 4.650012969970703\n",
            "Batch: 771, iteration took 0.4816, Loss: 4.506499290466309\n",
            "Batch: 772, iteration took 0.4697, Loss: 4.754118919372559\n",
            "Batch: 773, iteration took 0.4778, Loss: 4.659566879272461\n",
            "Batch: 774, iteration took 0.4459, Loss: 4.623717784881592\n",
            "Batch: 775, iteration took 0.4869, Loss: 4.648345470428467\n",
            "Batch: 776, iteration took 0.4512, Loss: 4.586778163909912\n",
            "Batch: 777, iteration took 0.4736, Loss: 4.676152229309082\n",
            "Batch: 778, iteration took 0.4546, Loss: 4.678640365600586\n",
            "Batch: 779, iteration took 0.4768, Loss: 4.572013854980469\n",
            "Batch: 780, iteration took 0.4598, Loss: 4.653848648071289\n",
            "avg test bits per pixel 4.7068\n",
            "Epoch: 1\n",
            "Batch: 0, iteration took 0.5660, Loss: 4.660519599914551\n",
            "Batch: 1, iteration took 0.4631, Loss: 4.62993860244751\n",
            "Batch: 2, iteration took 0.4512, Loss: 4.61173152923584\n",
            "Batch: 3, iteration took 0.4655, Loss: 4.673323631286621\n",
            "Batch: 4, iteration took 0.4446, Loss: 4.586874008178711\n",
            "Batch: 5, iteration took 0.4714, Loss: 4.693015098571777\n",
            "Batch: 6, iteration took 0.4766, Loss: 4.630331993103027\n",
            "Batch: 7, iteration took 0.4779, Loss: 4.605707168579102\n",
            "Batch: 8, iteration took 0.4507, Loss: 4.580201148986816\n",
            "Batch: 9, iteration took 0.4825, Loss: 4.678397178649902\n",
            "Batch: 10, iteration took 0.4787, Loss: 4.576418876647949\n",
            "Batch: 11, iteration took 0.4837, Loss: 4.616158962249756\n",
            "Batch: 12, iteration took 0.4854, Loss: 4.571554660797119\n",
            "Batch: 13, iteration took 0.4601, Loss: 4.715005397796631\n",
            "Batch: 14, iteration took 0.4834, Loss: 4.602206707000732\n",
            "Batch: 15, iteration took 0.4609, Loss: 4.66455602645874\n",
            "Batch: 16, iteration took 0.4814, Loss: 4.653141975402832\n",
            "Batch: 17, iteration took 0.4682, Loss: 4.656734943389893\n",
            "Batch: 18, iteration took 0.4647, Loss: 4.491518020629883\n",
            "Batch: 19, iteration took 0.4734, Loss: 4.799943923950195\n",
            "Batch: 20, iteration took 0.4503, Loss: 4.677465438842773\n",
            "Batch: 21, iteration took 0.4787, Loss: 4.647651672363281\n",
            "Batch: 22, iteration took 0.4491, Loss: 4.640825271606445\n",
            "Batch: 23, iteration took 0.4707, Loss: 4.651939392089844\n",
            "Batch: 24, iteration took 0.4748, Loss: 4.610255241394043\n",
            "Batch: 25, iteration took 0.4698, Loss: 4.609928131103516\n",
            "Batch: 26, iteration took 0.4642, Loss: 4.605892181396484\n",
            "Batch: 27, iteration took 0.4782, Loss: 4.6985368728637695\n",
            "Batch: 28, iteration took 0.4731, Loss: 4.639569282531738\n",
            "Batch: 29, iteration took 0.4960, Loss: 4.750158309936523\n",
            "Batch: 30, iteration took 0.4612, Loss: 4.630271911621094\n",
            "Batch: 31, iteration took 0.4450, Loss: 4.660747528076172\n",
            "Batch: 32, iteration took 0.4749, Loss: 4.571957588195801\n",
            "Batch: 33, iteration took 0.4661, Loss: 4.640155792236328\n",
            "Batch: 34, iteration took 0.4724, Loss: 4.630807399749756\n",
            "Batch: 35, iteration took 0.4803, Loss: 4.687337398529053\n",
            "Batch: 36, iteration took 0.4633, Loss: 4.645610809326172\n",
            "Batch: 37, iteration took 0.4457, Loss: 4.6861348152160645\n",
            "Batch: 38, iteration took 0.4477, Loss: 4.535953521728516\n",
            "Batch: 39, iteration took 0.4815, Loss: 4.697281360626221\n",
            "Batch: 40, iteration took 0.4750, Loss: 4.606207847595215\n",
            "Batch: 41, iteration took 0.4725, Loss: 4.684291362762451\n",
            "Batch: 42, iteration took 0.4572, Loss: 4.5482282638549805\n",
            "Batch: 43, iteration took 0.4772, Loss: 4.674244403839111\n",
            "Batch: 44, iteration took 0.4765, Loss: 4.652728080749512\n",
            "Batch: 45, iteration took 0.4796, Loss: 4.590534210205078\n",
            "Batch: 46, iteration took 0.4818, Loss: 4.564690589904785\n",
            "Batch: 47, iteration took 0.4737, Loss: 4.539559364318848\n",
            "Batch: 48, iteration took 0.4646, Loss: 4.629024028778076\n",
            "Batch: 49, iteration took 0.4510, Loss: 4.584689140319824\n",
            "Batch: 50, iteration took 0.4666, Loss: 4.650814533233643\n",
            "Batch: 51, iteration took 0.4764, Loss: 4.577272415161133\n",
            "Batch: 52, iteration took 0.4872, Loss: 4.539342403411865\n",
            "Batch: 53, iteration took 0.4646, Loss: 4.432093143463135\n",
            "Batch: 54, iteration took 0.4755, Loss: 4.489381790161133\n",
            "Batch: 55, iteration took 0.4517, Loss: 4.617497444152832\n",
            "Batch: 56, iteration took 0.4655, Loss: 4.483952522277832\n",
            "Batch: 57, iteration took 0.4835, Loss: 4.58847188949585\n",
            "Batch: 58, iteration took 0.4764, Loss: 4.691529273986816\n",
            "Batch: 59, iteration took 0.4802, Loss: 4.590634346008301\n",
            "Batch: 60, iteration took 0.4806, Loss: 4.754651069641113\n",
            "Batch: 61, iteration took 0.4822, Loss: 4.52268123626709\n",
            "Batch: 62, iteration took 0.4659, Loss: 4.597141265869141\n",
            "Batch: 63, iteration took 0.4695, Loss: 4.542460918426514\n",
            "Batch: 64, iteration took 0.4595, Loss: 4.5942792892456055\n",
            "Batch: 65, iteration took 0.4689, Loss: 4.5750017166137695\n",
            "Batch: 66, iteration took 0.4653, Loss: 4.596170902252197\n",
            "Batch: 67, iteration took 0.4454, Loss: 4.589899063110352\n",
            "Batch: 68, iteration took 0.4762, Loss: 4.667364120483398\n",
            "Batch: 69, iteration took 0.4458, Loss: 4.626079082489014\n",
            "Batch: 70, iteration took 0.4752, Loss: 4.743142127990723\n",
            "Batch: 71, iteration took 0.4483, Loss: 4.594307899475098\n",
            "Batch: 72, iteration took 0.4761, Loss: 4.710265636444092\n",
            "Batch: 73, iteration took 0.4552, Loss: 4.702522277832031\n",
            "Batch: 74, iteration took 0.4632, Loss: 4.726468086242676\n",
            "Batch: 75, iteration took 0.4642, Loss: 4.742402076721191\n",
            "Batch: 76, iteration took 0.4462, Loss: 4.5939741134643555\n",
            "Batch: 77, iteration took 0.4669, Loss: 4.637844562530518\n",
            "Batch: 78, iteration took 0.4770, Loss: 4.735492706298828\n",
            "Batch: 79, iteration took 0.4657, Loss: 4.7353620529174805\n",
            "Batch: 80, iteration took 0.4467, Loss: 4.605401039123535\n",
            "Batch: 81, iteration took 0.4528, Loss: 4.616935729980469\n",
            "Batch: 82, iteration took 0.4731, Loss: 4.611292362213135\n",
            "Batch: 83, iteration took 0.4690, Loss: 4.592947006225586\n",
            "Batch: 84, iteration took 0.4683, Loss: 4.70631217956543\n",
            "Batch: 85, iteration took 0.4465, Loss: 4.637495994567871\n",
            "Batch: 86, iteration took 0.4711, Loss: 4.6244425773620605\n",
            "Batch: 87, iteration took 0.4478, Loss: 4.5616607666015625\n",
            "Batch: 88, iteration took 0.4810, Loss: 4.560544013977051\n",
            "Batch: 89, iteration took 0.4759, Loss: 4.698062896728516\n",
            "Batch: 90, iteration took 0.4732, Loss: 4.634939193725586\n",
            "Batch: 91, iteration took 0.4474, Loss: 4.681529521942139\n",
            "Batch: 92, iteration took 0.4541, Loss: 4.790565013885498\n",
            "Batch: 93, iteration took 0.4724, Loss: 4.593185901641846\n",
            "Batch: 94, iteration took 0.4707, Loss: 4.635956764221191\n",
            "Batch: 95, iteration took 0.4779, Loss: 4.5358171463012695\n",
            "Batch: 96, iteration took 0.4850, Loss: 4.660605430603027\n",
            "Batch: 97, iteration took 0.4783, Loss: 4.602997303009033\n",
            "Batch: 98, iteration took 0.4641, Loss: 4.704093933105469\n",
            "Batch: 99, iteration took 0.4817, Loss: 4.562713146209717\n",
            "Batch: 100, iteration took 0.4721, Loss: 4.6136627197265625\n",
            "Batch: 101, iteration took 0.4825, Loss: 4.568169593811035\n",
            "Batch: 102, iteration took 0.4624, Loss: 4.597837448120117\n",
            "Batch: 103, iteration took 0.4813, Loss: 4.539417743682861\n",
            "Batch: 104, iteration took 0.4672, Loss: 4.521919250488281\n",
            "Batch: 105, iteration took 0.4574, Loss: 4.517223358154297\n",
            "Batch: 106, iteration took 0.4522, Loss: 4.566108226776123\n",
            "Batch: 107, iteration took 0.4813, Loss: 4.6165313720703125\n",
            "Batch: 108, iteration took 0.4893, Loss: 4.741153717041016\n",
            "Batch: 109, iteration took 0.4594, Loss: 4.544643402099609\n",
            "Batch: 110, iteration took 0.4526, Loss: 4.718440055847168\n",
            "Batch: 111, iteration took 0.4658, Loss: 4.602816581726074\n",
            "Batch: 112, iteration took 0.4490, Loss: 4.601459980010986\n",
            "Batch: 113, iteration took 0.4608, Loss: 4.584306240081787\n",
            "Batch: 114, iteration took 0.4551, Loss: 4.48139762878418\n",
            "Batch: 115, iteration took 0.4651, Loss: 4.68092679977417\n",
            "Batch: 116, iteration took 0.4635, Loss: 4.5388665199279785\n",
            "Batch: 117, iteration took 0.4553, Loss: 4.622035503387451\n",
            "Batch: 118, iteration took 0.4531, Loss: 4.4975738525390625\n",
            "Batch: 119, iteration took 0.4483, Loss: 4.51637601852417\n",
            "Batch: 120, iteration took 0.4839, Loss: 4.698785781860352\n",
            "Batch: 121, iteration took 0.4510, Loss: 4.530235290527344\n",
            "Batch: 122, iteration took 0.4788, Loss: 4.541613578796387\n",
            "Batch: 123, iteration took 0.4539, Loss: 4.587481498718262\n",
            "Batch: 124, iteration took 0.4485, Loss: 4.548570156097412\n",
            "Batch: 125, iteration took 0.4610, Loss: 4.697318077087402\n",
            "Batch: 126, iteration took 0.4613, Loss: 4.533331871032715\n",
            "Batch: 127, iteration took 0.4642, Loss: 4.651308536529541\n",
            "Batch: 128, iteration took 0.4493, Loss: 4.575460910797119\n",
            "Batch: 129, iteration took 0.4593, Loss: 4.532464027404785\n",
            "Batch: 130, iteration took 0.4519, Loss: 4.485301971435547\n",
            "Batch: 131, iteration took 0.4607, Loss: 4.516152381896973\n",
            "Batch: 132, iteration took 0.4539, Loss: 4.533212661743164\n",
            "Batch: 133, iteration took 0.4534, Loss: 4.520005226135254\n",
            "Batch: 134, iteration took 0.4839, Loss: 4.527836322784424\n",
            "Batch: 135, iteration took 0.4534, Loss: 4.513065338134766\n",
            "Batch: 136, iteration took 0.4824, Loss: 4.5858049392700195\n",
            "Batch: 137, iteration took 0.4617, Loss: 4.546228408813477\n",
            "Batch: 138, iteration took 0.4650, Loss: 4.535582065582275\n",
            "Batch: 139, iteration took 0.4628, Loss: 4.510828971862793\n",
            "Batch: 140, iteration took 0.4641, Loss: 4.601647853851318\n",
            "Batch: 141, iteration took 0.4748, Loss: 4.570781707763672\n",
            "Batch: 142, iteration took 0.4514, Loss: 4.6437578201293945\n",
            "Batch: 143, iteration took 0.4779, Loss: 4.53439998626709\n",
            "Batch: 144, iteration took 0.4545, Loss: 4.554471969604492\n",
            "Batch: 145, iteration took 0.4639, Loss: 4.5051469802856445\n",
            "Batch: 146, iteration took 0.4482, Loss: 4.507687091827393\n",
            "Batch: 147, iteration took 0.4673, Loss: 4.612702369689941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_k3NiiVxWFWG",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "Kingma,  D.  P.  &  Dhariwal,  P.  (2018),  Glow:  Generative  flow  with  invertible  1x1  convolutions, in ‘Advances in Neural Information Processing Systems’, pp. 10215–10224.\n",
        "\n",
        "Dinh, L., Sohl-Dickstein, J. & Bengio, S. (2016), ‘Density estimation using real nvp’ ,arXiv preprintarXiv:1605.08803.\n",
        "\n",
        "\n"
      ]
    }
  ]
}